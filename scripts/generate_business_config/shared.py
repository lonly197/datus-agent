#!/usr/bin/env python3
# Copyright 2025-present DatusAI, Inc.
# Licensed under the Apache License, Version 2.0.
# See http://www.apache.org/licenses/LICENSE-2.0 for details.

"""
Shared constants and utilities for business config generation.
"""

import re
from enum import IntEnum
from typing import Dict, List, Set, Optional


class TablePriority(IntEnum):
    """è¡¨ä¼˜å…ˆçº§æšä¸¾ï¼Œæ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜"""
    DIM = 1      # ç»´åº¦è¡¨ - æœ€é«˜ä¼˜å…ˆçº§
    DWD = 2      # æ˜ç»†äº‹å®è¡¨
    DWS = 3      # æ±‡æ€»äº‹å®è¡¨
    ADS = 4      # åº”ç”¨æ•°æ®è¡¨
    ODS = 5      # æ“ä½œæ•°æ®è¡¨ - æœ€ä½ä¼˜å…ˆçº§
    UNKNOWN = 99  # æœªçŸ¥ç±»å‹


# è¡¨å‰ç¼€åˆ°ä¼˜å…ˆçº§çš„æ˜ å°„
TABLE_PREFIX_PRIORITY: Dict[str, TablePriority] = {
    'dim_': TablePriority.DIM,
    'dwd_': TablePriority.DWD,
    'dws_': TablePriority.DWS,
    'ads_': TablePriority.ADS,
    'ods_': TablePriority.ODS,
}

# åœç”¨è¯åˆ—è¡¨ï¼ˆç”¨äºå…³é”®è¯æå–ï¼‰
STOP_WORDS: Set[str] = {
    "çš„", "å’Œ", "æˆ–", "ä¸", "åŠ", "æ˜¯", "åœ¨", "ç”¨äº", "è¡¨ç¤º", "æŒ‡",
    "å¯¹", "ä»", "åˆ°", "ä¸º", "æœ‰", "ç”±", "ç­‰", "å¯", "è¯·", "éœ€",
    "ä»¥", "æ ¹æ®", "æŒ‰ç…§", "ä¾æ®", "åŒ…æ‹¬", "åŒ…å«", "æ¶‰åŠ",
}

# å¸¸è§åŒä¹‰è¯æ˜ å°„ï¼ˆä¸šåŠ¡æœ¯è¯­æ ‡å‡†åŒ–ï¼‰
SYNONYM_MAP: Dict[str, str] = {
    "è½¦ç§": "è½¦å‹",
    "è½¦ç³»": "è½¦å‹ç³»åˆ—",
    "dealership": "ç»é”€åº—",
    "4såº—": "ç»é”€åº—",
    "é—¨åº—": "ç»é”€åº—",
}

# æŠ€æœ¯è¯æ±‡é›†åˆï¼ˆç”¨äºè¿‡æ»¤ï¼‰
TECHNICAL_TERMS: Set[str] = {
    'id', 'code', 'name', 'status', 'type', 'flag', 'time', 'date',
    'create', 'update', 'delete', 'insert', 'select', 'from', 'where',
    'table', 'column', 'field', 'index', 'key', 'value',
    'dealer_clue_code', 'original_clue_code', 'customer_id',
    'engine', 'key', 'duplicate', 'distributed', 'random', 'min', 'max', 'properties',
}

# å¸¸è§æŒ‡æ ‡åç¼€
METRIC_SUFFIXES: List[str] = [
    'æ•°é‡', 'æ•°', 'é‡', 'ç‡', 'å æ¯”', 'æ¯”ä¾‹', 'é‡‘é¢', 'æ¬¡æ•°', 'å¤©æ•°', 'æ—¶é•¿',
    'ç›®æ ‡', 'å®ç»©', 'åˆè®¡', 'æ±‡æ€»', 'ç»Ÿè®¡', 'å¹³å‡', 'æœ€å¤§', 'æœ€å°',
    'åŠæ—¶', 'å®Œæˆ', 'è¾¾æˆ', 'è½¬åŒ–', 'å˜æ›´', 'æ–°å¢', 'æ´»è·ƒ'
]

# å¸¸è§æŠ€æœ¯è¯æ±‡ï¼ˆç”¨äºå…³é”®è¯è¿‡æ»¤ï¼‰
TECHNICAL_KEYWORDS: Set[str] = {
    'æ˜ç»†', 'æ±‡æ€»', 'ç»Ÿè®¡', 'è®¡ç®—', 'ç»“æœ', 'æ•°æ®', 'ä¿¡æ¯', 'å­—æ®µ', 'è¡¨å',
}


def is_meaningful_term(term: str, min_length: int = 2) -> bool:
    """åˆ¤æ–­æœ¯è¯­æ˜¯å¦æœ‰ä¸šåŠ¡æ„ä¹‰
    
    Args:
        term: å¾…åˆ¤æ–­çš„æœ¯è¯­
        min_length: æœ€å°é•¿åº¦è¦æ±‚ï¼Œé»˜è®¤2
        
    Returns:
        bool: å¦‚æœæœ‰ä¸šåŠ¡æ„ä¹‰è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    if not term or len(term) < min_length:
        return False

    if term.lower() in TECHNICAL_TERMS:
        return False

    if re.match(r'^\d+$', term):
        return False

    if term.startswith('_'):
        return False

    return True


def get_table_priority(table_name: str) -> TablePriority:
    """æ ¹æ®è¡¨åè·å–ä¼˜å…ˆçº§
    
    Args:
        table_name: è¡¨å
        
    Returns:
        TablePriority: è¡¨çš„ä¼˜å…ˆçº§
    """
    if not table_name:
        return TablePriority.UNKNOWN
    
    table_lower = table_name.lower()
    for prefix, priority in TABLE_PREFIX_PRIORITY.items():
        if table_lower.startswith(prefix):
            return priority
    return TablePriority.UNKNOWN


def should_include_table(table_name: str, max_priority: TablePriority = TablePriority.ADS) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«è¯¥è¡¨
    
    Args:
        table_name: è¡¨å
        max_priority: æœ€å¤§å…è®¸çš„ä¼˜å…ˆçº§ï¼ˆé»˜è®¤ä¸ºADSï¼Œå³åŒ…å«DIM/DWD/DWS/ADSï¼Œæ’é™¤ODSï¼‰
        
    Returns:
        bool: å¦‚æœåº”è¯¥åŒ…å«è¿”å› True
    """
    priority = get_table_priority(table_name)
    return priority != TablePriority.UNKNOWN and priority <= max_priority


# ç”¨äºæ¸…æ´—æ–‡æœ¬çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
# æ³¨æ„ï¼šemojièŒƒå›´ä¸èƒ½ä¸CJKå­—ç¬¦èŒƒå›´ï¼ˆ\u4e00-\u9fffï¼‰é‡å 
TEXT_CLEANING_PATTERNS = {
    # ç§»é™¤emoji - ä½¿ç”¨æ˜ç¡®çš„emojièŒƒå›´ï¼Œé¿å…ä¸CJKå­—ç¬¦é‡å 
    'emoji': re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons: ğŸ˜€-ğŸ™
        "\U0001F300-\U0001F5FF"  # symbols & pictographs: ğŸŒ€-ğŸ—¿
        "\U0001F680-\U0001F6FF"  # transport & map: ğŸš€-ğŸ›¿
        "\U0001F1E0-\U0001F1FF"  # flags: ğŸ‡¦-ğŸ‡¿
        "\U00002702-\U000027B0"  # dingbats: âœ‚-â°
        "\U0001F900-\U0001F9FF"  # supplemental symbols: ğŸ¦€-ğŸ§¿
        "\U00002600-\U000026FF"  # misc symbols: â˜€-â›¿
        "\U0001F018-\U0001F270"  # æ›´å¤šemoji
        "\U00002300-\U000023FF"  # misc technical: âŒ€-â¿
        "]+",
        flags=re.UNICODE
    ),
    # ç§»é™¤è¡Œé¦–åºå·ï¼ˆå¦‚ 1.ã€â‘ ã€(1)ã€ï¼ˆ1ï¼‰ç­‰ï¼‰- ä»…åŒ¹é…è¡Œé¦–
    'numbered_list': re.compile(r'^[\s]*(?:\d+[\.ã€]|\([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)|ï¼ˆ[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰|[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³])[\s]*'),
    # è¡Œå†…åºå·æ ‡è®°ï¼ˆç”¨äºæ›¿æ¢ä¸ºç©ºæ ¼è€Œéåˆ é™¤ï¼‰
    'inline_number': re.compile(r'\([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)|ï¼ˆ[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰|[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]'),
    # å¤šä½™ç©ºæ ¼
    'extra_spaces': re.compile(r'\s+'),
    # å¤šä½™æ¢è¡Œ
    'extra_newlines': re.compile(r'\n+'),
    # ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
    'special_chars': re.compile(r'[*#^~|\\]'),
}


def clean_excel_text(text: Optional[str], remove_newlines: bool = False) -> str:
    """æ¸…æ´—Excelå•å…ƒæ ¼æ–‡æœ¬
    
    å¤„ç†å†…å®¹ï¼š
    - ç§»é™¤emoji
    - ç§»é™¤åºå·ï¼ˆå¦‚ 1.ã€â‘ ã€(1)ç­‰ï¼‰
    - è§„èŒƒåŒ–ç©ºæ ¼å’Œæ¢è¡Œ
    - ç§»é™¤ç‰¹æ®Šç¬¦å·
    - å»é™¤é¦–å°¾ç©ºç™½
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        remove_newlines: æ˜¯å¦ç§»é™¤æ‰€æœ‰æ¢è¡Œï¼ˆé»˜è®¤ä¿ç•™ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼ï¼‰
        
    Returns:
        str: æ¸…æ´—åçš„æ–‡æœ¬
    """
    if not text or not isinstance(text, str):
        return ""
    
    # ç§»é™¤emoji
    text = TEXT_CLEANING_PATTERNS['emoji'].sub('', text)
    
    # ç§»é™¤è¡Œé¦–åºå·æ ‡è®°
    text = TEXT_CLEANING_PATTERNS['numbered_list'].sub('', text)
    # å°†è¡Œå†…åºå·æ›¿æ¢ä¸ºç©ºæ ¼ï¼ˆé¿å…ä¸å‰åæ–‡å­—ç²˜è¿ï¼‰
    text = TEXT_CLEANING_PATTERNS['inline_number'].sub(' ', text)
    
    # ç§»é™¤ç‰¹æ®Šç¬¦å·
    text = TEXT_CLEANING_PATTERNS['special_chars'].sub('', text)
    
    # å¤„ç†æ¢è¡Œ
    if remove_newlines:
        text = text.replace('\n', ' ').replace('\r', ' ')
    else:
        # å°†å¤šä¸ªæ¢è¡Œæ›¿æ¢ä¸ºå•ä¸ª
        text = TEXT_CLEANING_PATTERNS['extra_newlines'].sub('\n', text)
    
    # è§„èŒƒåŒ–ç©ºæ ¼
    text = TEXT_CLEANING_PATTERNS['extra_spaces'].sub(' ', text)
    
    # å»é™¤é¦–å°¾ç©ºç™½
    text = text.strip()
    
    return text


def extract_clean_keywords(text: str, min_length: int = 2, max_length: int = 20) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–æ¸…æ´—åçš„å…³é”®è¯
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        min_length: æœ€å°é•¿åº¦
        max_length: æœ€å¤§é•¿åº¦
        
    Returns:
        List[str]: å…³é”®è¯åˆ—è¡¨
    """
    if not text:
        return []
    
    # å…ˆæ¸…æ´—æ–‡æœ¬
    cleaned = clean_excel_text(text, remove_newlines=True)
    if not cleaned:
        return []
    
    keywords = []
    
    # æå–ä¸­æ–‡è¯æ±‡
    for match in re.finditer(r'[\u4e00-\u9fa5]{' + str(min_length) + r',' + str(max_length) + r'}', cleaned):
        kw = match.group()
        if kw not in STOP_WORDS and is_meaningful_term(kw, min_length):
            keywords.append(kw)
    
    # æå–è‹±æ–‡/æ•°å­—ä¸šåŠ¡è¯æ±‡
    for match in re.finditer(r'[a-z_][a-z0-9_]{' + str(min_length - 1) + r',}', cleaned.lower()):
        kw = match.group()
        if kw not in TECHNICAL_TERMS and len(kw) <= 40:
            keywords.append(kw)
    
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_keywords = []
    for kw in keywords:
        if kw not in seen:
            seen.add(kw)
            unique_keywords.append(kw)
    
    return unique_keywords
