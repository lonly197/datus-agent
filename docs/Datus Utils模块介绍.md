# Datus Utils æ¨¡å—ä»‹ç»

> **æ–‡æ¡£ç‰ˆæœ¬**: v2.0
> **æ›´æ–°æ—¥æœŸ**: 2026-01-22
> **ç›¸å…³æ¨¡å—**: `datus/utils/`

---

## æ¨¡å—æ¦‚è¿°

### ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡ç†å¿µ

**"æ•°æ®å·¥ç¨‹åŸºç¡€è®¾æ–½å·¥å…·é›†"** - é€šè¿‡æ ‡å‡†åŒ–å·¥å…·é›†åˆå®ç°æ•°æ®å·¥ç¨‹åº”ç”¨çš„å®Œæ•´åŸºç¡€è®¾æ–½æ”¯æŒ

Datus Utilsæ¨¡å—é‡‡ç”¨**åˆ†å±‚å·¥å…·æ¶æ„**ï¼Œæ ¸å¿ƒè®¾è®¡ç†å¿µåŒ…æ‹¬ï¼š

1. **åŸºç¡€è®¾æ–½æŠ½è±¡**ï¼šå°†ç³»ç»Ÿçº§æ“ä½œæŠ½è±¡ä¸ºæ ‡å‡†æ¥å£
2. **æ•°æ®å¤„ç†æµæ°´çº¿**ï¼šæä¾›ç«¯åˆ°ç«¯çš„ä»æ•°æ®åˆ°ç»“æœçš„å¤„ç†èƒ½åŠ›
3. **ç¯å¢ƒè‡ªé€‚åº”**ï¼šæ ¹æ®è¿è¡Œç¯å¢ƒè‡ªåŠ¨è°ƒæ•´è¡Œä¸ºå’Œé…ç½®
4. **å¯é æ€§ä¿éšœ**ï¼šå†…ç½®é”™è¯¯å¤„ç†ã€ç›‘æ§å’Œæ€§èƒ½ä¼˜åŒ–

### ğŸ“Š æ¨¡å—ç›®å½•ç»“æ„

```
datus/utils/
â”œâ”€â”€ __init__.py               # æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ constants.py             # å¸¸é‡å®šä¹‰ï¼ˆæ•°æ®åº“ã€LLMã€SQLç±»å‹ï¼‰
â”œâ”€â”€ exceptions.py            # å¼‚å¸¸ä½“ç³»ï¼ˆErrorCodeã€DatusExceptionï¼‰
â”œâ”€â”€ async_utils.py           # å¼‚æ­¥è¿è¡Œæ—¶ç®¡ç†
â”œâ”€â”€ loggings.py             # ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ
â”œâ”€â”€ path_manager.py         # é›†ä¸­å¼è·¯å¾„ç®¡ç†
â”œâ”€â”€ sub_agent_manager.py    # Sub-Agent ç®¡ç†
â”œâ”€â”€ sql_utils.py             # SQL å¤„ç†å·¥å…·ï¼ˆDDLè§£æã€éªŒè¯ï¼‰
â”œâ”€â”€ json_utils.py            # JSON æ•°æ®å¤„ç†
â”œâ”€â”€ token_utils.py           # Token è®¡ç®—å·¥å…·
â”œâ”€â”€ compress_utils.py        # æ•°æ®å‹ç¼©å·¥å…·
â”œâ”€â”€ benchmark_utils.py       # SQL åŸºå‡†æµ‹è¯•å·¥å…·
â”œâ”€â”€ error_handling.py        # ç»Ÿä¸€é”™è¯¯å¤„ç†
â”œâ”€â”€ traceable_utils.py      # å¯è¿½è¸ªè£…é¥°å™¨
â”œâ”€â”€ text_utils.py            # æ–‡æœ¬æ¸…ç†å·¥å…·
â”œâ”€â”€ device_utils.py          # è®¾å¤‡æ£€æµ‹
â”œâ”€â”€ env.py                   # ç¯å¢ƒå˜é‡ç®¡ç†
â”œâ”€â”€ csv_utils.py             # CSV å¤„ç†å·¥å…·
â”œâ”€â”€ pyarrow_utils.py         # PyArrow å·¥å…·
â”œâ”€â”€ schema_utils.py          # Schema å¤„ç†å·¥å…·
â”œâ”€â”€ time_utils.py            # æ—¶é—´å¤„ç†å·¥å…·
â”œâ”€â”€ typing_fix.py            # ç±»å‹å…¼å®¹ä¿®å¤
â””â”€â”€ context_lock.py          # ä¸Šä¸‹æ–‡é”
```

---

## æ ¸å¿ƒç»„ä»¶

### 1. å¸¸é‡å®šä¹‰ (constants.py)

```python
class DBType(str, Enum):
    """æ”¯æŒçš„æ•°æ®åº“ç±»å‹"""
    SQLITE = "sqlite"
    DUCKDB = "duckdb"
    MYSQL = "mysql"
    POSTGRESQL = "postgresql"
    POSTGRES = "postgres"
    SNOWFLAKE = "snowflake"
    CLICKHOUSE = "clickhouse"
    BIGQUERY = "bigquery"
    STARROCKS = "starrocks"
    SQLSERVER = "sqlserver"
    MSSQL = "mssql"
    ORACLE = "oracle"
    HIVE = "hive"
    CLICKZETTA = "clickzetta"

    @classmethod
    def support_catalog(cls, db_type: str) -> bool
    @classmethod
    def support_database(cls, db_type: str) -> bool
    @classmethod
    def support_schema(cls, db_type: str) -> bool


class LLMProvider(str, Enum):
    """æ”¯æŒçš„ LLM æä¾›å•†"""
    OPENAI = "openai"
    CLAUDE = "claude"
    DEEPSEEK = "deepseek"
    QWEN = "qwen"
    GLM = "glm"
    ANTHROPIC = "anthropic"
    GEMINI = "gemini"
    LLAMA = "llama"
    GPT = "gpt"


class SQLType(str, Enum):
    """SQL è¯­å¥ç±»å‹"""
    SELECT = "select"
    INSERT = "insert"
    UPDATE = "update"
    DELETE = "delete"
    MERGE = "merge"
    DDL = "ddl"
    METADATA_SHOW = "metadata"
    EXPLAIN = "explain"
    CONTENT_SET = "context_set"
    UNKNOWN = "unknown"


# ç³»ç»Ÿå†…ç½® Sub-Agent
SYS_SUB_AGENTS = {"gen_semantic_model", "gen_metrics", "gen_sql_summary"}
```

**æ•°æ®åº“æ–¹è¨€æ”¯æŒï¼š**
- **SUPPORT_CATALOG_DIALECTS**: StarRocks, Snowflake, BigQuery
- **SUPPORT_DATABASE_DIALECTS**: é™¤ SQLite å¤–çš„æ‰€æœ‰æ•°æ®åº“
- **SUPPORT_SCHEMA_DIALECTS**: Snowflake, BigQuery, MSSQL, Oracle, DuckDB, PostgreSQL

---

### 2. å¼‚å¸¸ä½“ç³» (exceptions.py)

#### 2.1 ErrorCode æšä¸¾

```python
class ErrorCode(Enum):
    """7ä½é”™è¯¯ç ä½“ç³»ï¼šç±»åˆ«(2ä½) + å­ç±»(2ä½) + åºå·(3ä½)"""

    # é€šç”¨é”™è¯¯ (10xxxx)
    COMMON_UNKNOWN = ("1000000", "Unknown error occurred")
    COMMON_FIELD_INVALID = ("1000001", "{field_name} invalid")
    COMMON_FILE_NOT_FOUND = ("100002", "{config_name} not found: {file_name}")
    COMMON_FIELD_REQUIRED = ("100003", "Missing required field: {field_name}")

    # èŠ‚ç‚¹æ‰§è¡Œé”™è¯¯ (20xxxx)
    NODE_EXECUTION_FAILED = ("200001", "Node execution failed")
    NODE_NO_SQL_CONTEXT = ("200002", "No SQL context available")

    # æ¨¡å‹é”™è¯¯ (30xxxx)
    MODEL_REQUEST_FAILED = ("300001", "LLM request failed")
    MODEL_TIMEOUT = ("300003", "Model request timeout")
    MODEL_AUTHENTICATION_ERROR = ("300011", "Authentication failed (HTTP 401)")
    MODEL_PERMISSION_ERROR = ("300012", "API key lacks permissions (HTTP 403)")
    MODEL_RATE_LIMIT = ("300015", "Rate limit exceeded (HTTP 429)")

    # å·¥å…·é”™è¯¯ (40xxxx)
    TOOL_EXECUTION_FAILED = ("400001", "Tool execution failed")

    # å­˜å‚¨é”™è¯¯ (41xxxx)
    STORAGE_CONNECTION_FAILED = ("410001", "Failed to connect to vector database")
    STORAGE_SEARCH_FAILED = ("410004", "Vector search failed")

    # æ•°æ®åº“é”™è¯¯ (50xxxx)
    DB_CONNECTION_FAILED = ("500001", "Failed to establish connection")
    DB_EXECUTION_TIMEOUT = ("500007", "Query execution timed out")
```

#### 2.2 DatusException ç±»

```python
class DatusException(Exception):
    """Datus è‡ªå®šä¹‰å¼‚å¸¸"""

    def __init__(
        self,
        code: ErrorCode,
        message: Optional[str] = None,
        message_args: Optional[dict[str, Any]] = None,
        *args: object,
    ):
        self.code = code
        self.message_args = message_args or {}
        self.message = self.build_msg(message, message_args)
        super().__init__(self.message, *args)

    def build_msg(self, message: Optional[str], message_args: Optional[dict]) -> str:
        """æ„å»ºé”™è¯¯æ¶ˆæ¯"""
        if message:
            final_message = message
        elif message_args:
            final_message = self.code.desc.format(**message_args)
        else:
            final_message = self.code.desc
        return f"error_code={self.code.code}, error_message={final_message}"
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# ä½¿ç”¨é¢„å®šä¹‰é”™è¯¯ç 
raise DatusException(
    ErrorCode.COMMON_FIELD_REQUIRED,
    message_args={"field_name": "api_key"}
)
# è¾“å‡º: error_code=100003, error_message=Missing required field: api_key

# è‡ªå®šä¹‰æ¶ˆæ¯
raise DatusException(
    ErrorCode.DB_CONNECTION_FAILED,
    message="Custom connection error message"
)
```

#### 2.3 å…¨å±€å¼‚å¸¸å¤„ç†å™¨

```python
def setup_exception_handler(console_logger=None, prefix_wrap_func=None):
    """è®¾ç½®å…¨å±€å¼‚å¸¸å¤„ç†å™¨

    è‡ªåŠ¨æ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•åˆ°æ—¥å¿—ç³»ç»Ÿ
    """
    sys.excepthook = global_exception_handler
```

---

### 3. å¼‚æ­¥è¿è¡Œæ—¶ (async_utils.py)

```python
def run_async(coro: Awaitable[T]) -> T:
    """
    åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥åç¨‹

    æ™ºèƒ½æ£€æµ‹è¿è¡Œç¯å¢ƒå¹¶é€‰æ‹©æ‰§è¡Œç­–ç•¥ï¼š
    - åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ï¼šä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ
    - åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­ï¼šåˆ›å»ºæ–°äº‹ä»¶å¾ªç¯
    """
    if loop and loop.is_running():
        # ä½¿ç”¨çº¿ç¨‹æ± 
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(asyncio.run, coro)
            return future.result()
    else:
        # ç›´æ¥è¿è¡Œ
        return asyncio.run(coro)


async def await_cancellable(coro: Awaitable[T], timeout: Optional[float] = None) -> T:
    """
    ç­‰å¾…å¯å–æ¶ˆçš„åç¨‹ï¼ˆä¸å±è”½å–æ¶ˆä¿¡å·ï¼‰
    """
    if timeout:
        return await asyncio.wait_for(coro, timeout=timeout)
    else:
        return await coro
```

---

### 4. æ—¥å¿—ç³»ç»Ÿ (loggings.py)

#### 4.1 DynamicLogManager

```python
class DynamicLogManager:
    """æ”¯æŒè¿è¡Œæ—¶åˆ‡æ¢è¾“å‡ºç›®æ ‡çš„åŠ¨æ€æ—¥å¿—ç®¡ç†å™¨"""

    def __init__(self, debug=False, log_dir=None):
        # è‡ªåŠ¨æ£€æµ‹æ—¥å¿—ç›®å½•
        if log_dir is None:
            if _is_source_environment():
                log_dir = "./logs"
            else:
                log_dir = str(get_path_manager().logs_dir)

    def set_output_target(self, target: Literal["both", "file", "console", "none"]):
        """è®¾ç½®æ—¥å¿—è¾“å‡ºç›®æ ‡"""

    @contextmanager
    def temporary_output(self, target: Literal["both", "file", "console", "none"]):
        """ä¸´æ—¶åˆ‡æ¢è¾“å‡ºç›®æ ‡çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# ä¸´æ—¶åªè¾“å‡ºåˆ°æ–‡ä»¶
with log_context("file"):
    logger.info("æ­¤æ—¥å¿—åªä¼šè¾“å‡ºåˆ°æ–‡ä»¶")

# é…ç½®æ—¥å¿—
configure_logging(debug=True, log_dir="./logs", console_output=True)
```

#### 4.2 ç»“æ„åŒ–æ—¥å¿—

ä½¿ç”¨ `structlog` é…ç½®ï¼š
- è‡ªåŠ¨æ·»åŠ ä»£ç ä½ç½® (`fileno`)
- å¼‚å¸¸ä¿¡æ¯è¿½è¸ª (`exc_info`)
- å½©è‰²æ§åˆ¶å°è¾“å‡ºï¼ˆæ–‡ä»¶ä¸­è‡ªåŠ¨ç§»é™¤é¢œè‰²ä»£ç ï¼‰

---

### 5. SQL å¤„ç†å·¥å…· (sql_utils.py)

#### 5.1 é¢„ç¼–è¯‘æ­£åˆ™æ¨¡å¼

```python
# æ€§èƒ½ä¼˜åŒ–ï¼šæ¨¡å—çº§é¢„ç¼–è¯‘æ‰€æœ‰æ­£åˆ™è¡¨è¾¾å¼
_TABLE_NAME_RE = re.compile(
    r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?[`"]?([\w.]+)[`"]?\s*\(',
    re.IGNORECASE
)
_COLUMN_COMMENT_RE = re.compile(
    r"COMMENT\s*(?:=\s*)?['\"]([^'\"]+)['\"]",
    re.IGNORECASE
)
```

#### 5.2 è¾“å…¥éªŒè¯

```python
def validate_sql_input(sql: Any, max_length: int = MAX_SQL_LENGTH) -> Tuple[bool, str]:
    """
    éªŒè¯ SQL è¾“å…¥çš„å®‰å…¨æ€§å’Œæ­£ç¡®æ€§

    é˜²æ­¢ï¼š
    - ReDoS æ”»å‡»ï¼ˆé€šè¿‡æ‹¬å·æ·±åº¦é™åˆ¶ï¼‰
    - å†…å­˜è€—å°½ï¼ˆé€šè¿‡é•¿åº¦é™åˆ¶ï¼‰
    - NULL å­—èŠ‚æ³¨å…¥
    """
    # ç±»å‹éªŒè¯
    if not isinstance(sql, str):
        return False, f"SQL must be a string, got {type(sql).__name__}"

    # é•¿åº¦éªŒè¯
    if len(sql) > max_length:
        return False, f"SQL length ({len(sql)}) exceeds maximum ({max_length})"

    # æ‹¬å·æ·±åº¦æ£€æŸ¥ï¼ˆé˜²æ­¢ ReDoSï¼‰
    paren_depth = 0
    max_paren_depth = 100
```

#### 5.3 ä¸­æ–‡æ³¨é‡Šä¿æŠ¤

```python
def _preserve_chinese_comments(sql: str) -> tuple:
    """ä¿æŠ¤ä¸­æ–‡æ³¨é‡Šæ–‡æœ¬åœ¨ DDL æ¸…ç†è¿‡ç¨‹ä¸­ä¸è¢«ç§»é™¤

    StarRocks ä½¿ç”¨åŒå¼•å·åŒ…è£¹ä¸­æ–‡ COMMENT æ–‡æœ¬
    """
    pattern = re.compile(r'COMMENT\s*(?:=\s*)?"([^"]*)"', re.IGNORECASE)

    def replace_with_placeholder(match):
        placeholder = f"__CHINESE_COMMENT_{len(protected)}__"
        protected[placeholder] = match.group(0)
        return f'COMMENT "{placeholder}"'

    return pattern.sub(replace_with_placeholder, sql), protected
```

#### 5.4 DDL è§£æ

```python
def parse_metadata_from_ddl(sql: str, dialect: str = DBType.SNOWFLAKE) -> Dict[str, Any]:
    """è§£æ CREATE TABLE è¯­å¥

    ä½¿ç”¨ sqlglot è¿›è¡Œä¸»è¦è§£æï¼Œæ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºå›é€€
    """
    dialect = parse_dialect(dialect)
    parsed = sqlglot.parse_one(sql.strip(), dialect=dialect, error_level=sqlglot.ErrorLevel.IGNORE)

    # æå–è¡¨ä¿¡æ¯ã€åˆ—ä¿¡æ¯ã€ä¸»é”®ã€å¤–é”®ã€ç´¢å¼•ç­‰


def extract_enhanced_metadata_from_ddl(sql: str, dialect: str) -> Dict[str, Any]:
    """ä» DDL ä¸­æå–å®Œæ•´çš„å…ƒæ•°æ®ï¼ˆå¤šç­–ç•¥è§£æï¼‰"""
    # Strategy 1: åŸå§‹ SQL
    # Strategy 2: æ¸…ç†åçš„ SQL
    # Strategy 3: ä¸åŒæ–¹è¨€
    # Fallback: æ­£åˆ™è¡¨è¾¾å¼è§£æ
```

#### 5.5 SQL ç±»å‹æ£€æµ‹

```python
def parse_sql_type(sql: str, dialect: str = DBType.SNOWFLAKE) -> SQLType:
    """
    è§£æ SQL è¯­å¥ç±»å‹

    æ”¯æŒï¼šSELECT, INSERT, UPDATE, DELETE, MERGE, DDL, EXPLAIN, SHOW, DESCRIBE, USE, SET
    """
    first_keyword_match = re.match(r'^\s*([A-Za-z]+)', sql_clean, re.IGNORECASE)
    first_keyword = first_keyword_match.group(1).upper()

    keyword_map = {
        'SELECT': SQLType.SELECT,
        'INSERT': SQLType.INSERT,
        'CREATE': SQLType.DDL,
        'SHOW': SQLType.METADATA_SHOW,
        'EXPLAIN': SQLType.EXPLAIN,
        # ...
    }
```

#### 5.6 å…ƒæ•°æ®æ ‡è¯†ç¬¦

```python
def metadata_identifier(
    dialect: str,
    catalog_name: str = "",
    database_name: str = "",
    schema_name: str = "",
    table_name: str = ""
) -> str:
    """
    åˆ›å»ºæ•°æ®åº“è¡¨çš„å”¯ä¸€æ ‡è¯†ç¬¦

    æ ¼å¼ï¼šcatalog.database.schema.table
    ç©ºç»„ä»¶ä¼šåˆ›å»ºè¿ç»­çš„ç‚¹ï¼ˆä¾‹å¦‚ï¼š"catalog.database..table"ï¼‰
    """
    parts = []
    if table_name:
        parts.append(table_name)
    if schema_name:
        parts.insert(0, schema_name)
    if database_name:
        parts.insert(0, database_name)
    if catalog_name:
        parts.insert(0, catalog_name)

    return ".".join(parts)
```

---

### 6. JSON æ•°æ®å¤„ç† (json_utils.py)

#### 6.1 LLM ç»“æœè§£æ

```python
def llm_result2json(llm_str: str, expected_type: type[Dict | List] = dict) -> Union[Dict, List, None]:
    """
    å°† LLM è¾“å‡ºå­—ç¬¦ä¸²è½¬æ¢ä¸º JSON å¯¹è±¡æˆ–æ•°ç»„

    æ”¯æŒæ ¼å¼ï¼š
    1. çº¯ JSON å­—ç¬¦ä¸²
    2. ```json ... ``` ä»£ç å—
    3. ``` ... ``` ä»£ç å—

    è‡ªåŠ¨ä¿®å¤æŸåçš„ JSON
    """
    try:
        cleaned_string = strip_json_str(llm_str)
        result = json_repair.loads(cleaned_string)

        # éªŒè¯æœ‰æ„ä¹‰çš„å†…å®¹
        if isinstance(result, dict):
            metadata_fields = {"fallback", "error", "traceback", "raw_response"}
            has_any_content = any(
                _has_content(result.get(key))
                for key in result.keys()
                if key not in metadata_fields
            )
            if not has_any_content:
                return None

        return result
    except (json.JSONDecodeError, ValueError, AttributeError, TypeError):
        return None


def llm_result2sql(llm_str: str) -> Optional[str]:
    """
    ä» LLM è¾“å‡ºä¸­æå– SQL

    æŸ¥æ‰¾ ```sql ... ``` æˆ– ```SQL ... ``` ä»£ç å—
    å›é€€ï¼šæŸ¥æ‰¾åŒ…å« SQL å…³é”®å­—çš„ä»£ç å—
    """
    sql_pattern = r"```(?:sql|SQL)\s*\n?(.*?)\n?```"
    match = re.search(sql_pattern, llm_str, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
```

#### 6.2 æ•°æ®æ ¼å¼è½¬æ¢

```python
def json2csv(result: Any, columns: Optional[List[str]] = None) -> str:
    """å°† JSON æ•°æ®è½¬æ¢ä¸º CSV æ ¼å¼"""
    if isinstance(result, str):
        if result.strip().startswith("[") or result.strip().startswith("{"):
            result = json_repair.loads(result)

    df = pd.DataFrame(result)
    output = StringIO()
    df.to_csv(output, index=False, columns=columns)
    return output.getvalue()


def json_list2markdown_table(json_list: List[Dict[str, Any]]) -> str:
    """å°†å­—å…¸åˆ—è¡¨è½¬æ¢ä¸º Markdown è¡¨æ ¼"""
    df = pd.DataFrame(json_list)
    return df.to_markdown()
```

#### 6.3 æ•°æ®è§„èŒƒåŒ–

```python
def _normalize_for_json(data: Any) -> Any:
    """
    å°†å„ç§ Python/Pydantic/pandas/NumPy å¯¹è±¡è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–ç»“æ„
    """
    # æ”¯æŒçš„ç±»å‹ï¼š
    # - datetime, date, time â†’ ISO æ ¼å¼å­—ç¬¦ä¸²
    # - Decimal â†’ å­—ç¬¦ä¸²
    # - UUID â†’ å­—ç¬¦ä¸²
    # - Pydantic BaseModel â†’ model_dump()
    # - pandas DataFrame â†’ å­—å…¸åˆ—è¡¨
    # - NumPy æ•°ç»„ â†’ åˆ—è¡¨
    # - dataclass â†’ asdict()
```

---

### 7. è·¯å¾„ç®¡ç† (path_manager.py)

```python
class DatusPathManager:
    """é›†ä¸­å¼ .datus ç›®å½•è·¯å¾„ç®¡ç†å™¨"""

    def __init__(self, datus_home: Optional[str] = None):
        if datus_home:
            self._datus_home = Path(datus_home).expanduser().resolve()
        else:
            self._datus_home = Path.home() / ".datus"

    @property
    def conf_dir(self) -> Path:
        """é…ç½®ç›®å½•: ~/.datus/conf"""
        return self._datus_home / "conf"

    @property
    def data_dir(self) -> Path:
        """æ•°æ®ç›®å½•: ~/.datus/data"""
        return self._datus_home / "data"

    @property
    def logs_dir(self) -> Path:
        """æ—¥å¿—ç›®å½•: ~/.datus/logs"""
        return self._datus_home / "logs"

    @property
    def sessions_dir(self) -> Path:
        """ä¼šè¯ç›®å½•: ~/.datus/sessions"""
        return self._datus_home / "sessions"

    # ... æ›´å¤šç›®å½•å±æ€§

    def rag_storage_path(self, namespace: str) -> Path:
        """RAG å­˜å‚¨è·¯å¾„"""
        return self.data_dir / f"datus_db_{namespace}"

    def sub_agent_path(self, agent_name: str) -> Path:
        """Sub-Agent å­˜å‚¨è·¯å¾„"""
        return self.data_dir / "sub_agents" / agent_name
```

**å…¨å±€å•ä¾‹ï¼š**
```python
def get_path_manager(datus_home: Optional[Path] = None) -> DatusPathManager:
    """è·å–å…¨å±€è·¯å¾„ç®¡ç†å™¨å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨å•ä¾‹ï¼‰"""
    global _path_manager
    if _path_manager is None:
        with _path_manager_lock:
            if _path_manager is None:
                _path_manager = DatusPathManager(datus_home)
    return _path_manager
```

---

### 8. Sub-Agent ç®¡ç† (sub_agent_manager.py)

```python
class SubAgentManager:
    """Sub-Agent é…ç½®å’Œæç¤ºç®¡ç†æ“ä½œå°è£…"""

    def list_agents(self) -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰ Sub-Agent é…ç½®"""

    def get_agent(self, agent_name: str) -> Optional[Dict[str, Any]]:
        """è·å–ç‰¹å®š Sub-Agent é…ç½®"""

    def save_agent(self, config: SubAgentConfig, previous_name: Optional[str] = None) -> Dict[str, Any]:
        """æŒä¹…åŒ– Sub-Agent é…ç½®

        å¤„ç†ï¼š
        - ä½œç”¨åŸŸçŸ¥è¯†åº“åˆ›å»º/é‡å‘½å/æ¸…é™¤
        - æç¤ºæ¨¡æ¿å¤åˆ¶/ç§»é™¤
        """

    def remove_agent(self, agent_name: str) -> bool:
        """åˆ é™¤ Sub-Agent"""

    def bootstrap_agent(
        self,
        config: SubAgentConfig,
        *,
        components: Optional[Sequence[str]] = None,
        strategy: SubAgentBootstrapStrategy = "overwrite",
    ) -> BootstrapResult:
        """å¼•å¯¼ Sub-Agentï¼ˆåˆ›å»ºä½œç”¨åŸŸçŸ¥è¯†åº“ï¼‰"""
```

---

### 9. æ•°æ®å‹ç¼© (compress_utils.py)

```python
class DataCompressor:
    """NL2SQL Agent æŸ¥è¯¢ç»“æœæ•°æ®å‹ç¼©å™¨"""

    def __init__(
        self,
        model_name: str = "gpt-3.5-turbo",
        token_threshold: int = 1024,
        tolerance_ratio: float = 0.1,
        output_format: Literal["table", "csv"] = "csv",
    ):
        """
        åˆå§‹åŒ–æ•°æ®å‹ç¼©å™¨

        model_name: æ”¯æŒæœ€æ–° (o200k_base) å’Œç”Ÿäº§ (cl100k_base) æ¨¡å‹
        """
        try:
            self.tokenizer = tiktoken.encoding_for_model(model_name)
        except Exception:
            self.tokenizer = None

    def compress(self, data: Union[List[Dict], pd.DataFrame, pa.Table]) -> Dict:
        """
        å‹ç¼©æ•°æ®å¹¶è¿”å›ç»“æœ

        å‹ç¼©ç­–ç•¥ï¼š
        - è¡Œå‹ç¼©ï¼š>20 è¡Œæ—¶ï¼Œå–å‰ 10 è¡Œå’Œå 10 è¡Œ
        - åˆ—å‹ç¼©ï¼šä¿ç•™ ID å’Œæ—¶é—´åˆ—ï¼Œä»ä¸­é—´ç§»é™¤å…¶ä»–åˆ—
        - æ··åˆå‹ç¼©ï¼šåŒæ—¶åº”ç”¨è¡Œå’Œåˆ—å‹ç¼©
        """

        return {
            "original_rows": original_rows,
            "original_columns": original_columns,
            "is_compressed": is_compressed,
            "compressed_data": compressed_data,
            "removed_columns": removed_columns,
            "compression_type": compression_type,
        }

    @classmethod
    def quick_compress(
        cls,
        data: Union[List[Dict], pd.DataFrame, pa.Table],
        model_name: str = "gpt-3.5-turbo",
        token_threshold: int = 1024,
        output_format: Literal["table", "csv"] = "csv",
    ) -> str:
        """å¿«é€Ÿå‹ç¼©æ–¹æ³•ï¼ˆä¸€æ¬¡æ€§ä½¿ç”¨ï¼‰"""
```

---

### 10. åŸºå‡†æµ‹è¯• (benchmark_utils.py)

```python
@dataclass
class WorkflowArtifacts:
    """å·¥ä½œæµäº§ç‰©æ•°æ®ç±»"""
    files: list[str]
    reference_sqls: list[str]
    reference_sql_names: list[str]
    semantic_models: list[str]
    metrics_names: list[str]


@dataclass
class ComparisonOutcome:
    """SQL æ‰§è¡Œæ¯”è¾ƒç»“æœ"""
    match_rate: float = 0.0
    matched_columns: list[tuple[str, str]] = field(default_factory=list)
    column_match_details: list[dict] = field(default_factory=list)
    value_match_details: list[dict] = field(default_factory=list)


class SQLComparator:
    """SQL æ‰§è¡Œç»“æœæ¯”è¾ƒå™¨"""

    def compare_csv_results(
        self,
        gold_standard: pd.DataFrame,
        actual_result: pd.DataFrame,
        ignore_order: bool = True,
        ignore_case: bool = True,
    ) -> ComparisonOutcome:
        """
        æ¯”è¾ƒ CSV ç»“æœä¸é‡‘æ ‡å‡†

        åˆ—çº§åŒ¹é…åˆ†æï¼š
        - ç²¾ç¡®åˆ—åŒ¹é…ï¼šåç§°å’Œæ•°æ®ç±»å‹éƒ½åŒ¹é…
        - å€¼åŒ¹é…ï¼šæ•°å€¼å®¹å·®æ¯”è¾ƒã€å­—ç¬¦ä¸²å¿½ç•¥å¤§å°å†™
        """

    def calculate_accuracy_metrics(
        self,
        outcomes: List[ComparisonOutcome]
    ) -> Dict[str, float]:
        """è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡ã€F1 åˆ†æ•°ï¼‰"""
```

---

### 11. ç»Ÿä¸€é”™è¯¯å¤„ç† (error_handling.py)

```python
class NodeErrorResult(BaseResult):
    """ç»Ÿä¸€èŠ‚ç‚¹é”™è¯¯ç»“æœ"""

    def __init__(
        self,
        success: bool = False,
        error_code: str = "",
        error_message: str = "",
        error_details: Optional[Dict[str, Any]] = None,
        node_context: Optional[Dict[str, Any]] = None,
        retryable: bool = False,
        recovery_suggestions: Optional[List[str]] = None,
    ):
        # é”™è¯¯ç ã€æ¶ˆæ¯ã€è¯¦æƒ…ã€ä¸Šä¸‹æ–‡ã€å¯é‡è¯•æ€§ã€æ¢å¤å»ºè®®


def unified_error_handler(node_type: str, operation: str):
    """
    ç»Ÿä¸€é”™è¯¯å¤„ç†è£…é¥°å™¨

    è‡ªåŠ¨å¤„ç†ï¼š
    - DatusExceptionï¼šè®°å½•å¹¶é‡æ–°æŠ›å‡º
    - JSONDecodeErrorï¼šåˆ›å»ºæ ‡å‡†åŒ–é”™è¯¯ç»“æœ
    - ConnectionError/TimeoutErrorï¼šæ ‡è®°ä¸ºå¯é‡è¯•
    - å…¶ä»–å¼‚å¸¸ï¼šè®°å½•å †æ ˆè·Ÿè¸ª
    """
```

---

### 12. å¯è¿½è¸ªè£…é¥°å™¨ (traceable_utils.py)

```python
def optional_traceable(name: str = "", run_type: RUN_TYPE_T = "chain"):
    """
    å¯é€‰çš„å¯è¿½è¸ªè£…é¥°å™¨

    å½“ LangSmith å¯ç”¨æ—¶è‡ªåŠ¨åŒ…è£…å‡½æ•°
    """
    def decorator(func):
        if not HAS_LANGSMITH:
            return func
        try:
            from langsmith import traceable

            trace_name = name or getattr(func, "__name__", "agent_operation")
            return traceable(name=trace_name, run_type=run_type)(func)
        except ImportError:
            return func

    return decorator


def create_openai_client(
    cls: Type[Union[OpenAI, AsyncOpenAI]],
    api_key: str,
    base_url: str,
    default_headers: Union[dict[str, str], None] = None,
    timeout: float = 300.0,
) -> Union[OpenAI, AsyncOpenAI]:
    """åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼ˆç¦ç”¨å†…ç½®é‡è¯•ï¼‰"""
```

---

### 13. å…¶ä»–å·¥å…·æ¨¡å—

#### 13.1 æ–‡æœ¬æ¸…ç† (text_utils.py)

```python
def clean_text(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬ï¼ˆUnicode è§„èŒƒåŒ–ã€ç§»é™¤ä¸å¯è§å­—ç¬¦ã€ç»Ÿä¸€æ¢è¡Œç¬¦ï¼‰
    """
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("\u00a0", " ").replace("\u200b", "").replace("\ufeff", "")
    text = re.sub(r"[\x00-\x08\x0B-\x1F\x7F]", "", text)
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    return text.strip()


def strip_markdown_code_block(text: str) -> str:
    """ç§»é™¤ Markdown ä»£ç å—æ ‡è®°"""
```

#### 13.2 Token å·¥å…· (token_utils.py)

```python
def get_encoding():
    """è·å– tiktoken ç¼–ç å™¨ï¼ˆcl100k_baseï¼‰"""
    global _encoding
    if _encoding is None:
        _encoding = tiktoken.get_encoding("cl100k_base")
    return _encoding


def cal_task_size(count: int, step: int) -> int:
    """è®¡ç®—ä»»åŠ¡åˆ†å‰²æ•°é‡"""
    return int(round(count / step + 0.5, 0))


def cal_gpt_tokens(text, encoding=None) -> int:
    """è®¡ç®— GPT token æ•°é‡"""
```

---

## æ¶æ„ç‰¹æ€§

### 1. ç¯å¢ƒæ„ŸçŸ¥

**è‡ªåŠ¨æ£€æµ‹æºç ç¯å¢ƒï¼š**
```python
def _is_source_environment() -> bool:
    """æ£€æŸ¥æ˜¯å¦ä»æºç ç›®å½•è¿è¡Œ"""
    has_pyproject = os.path.exists(os.path.join(project_root, "pyproject.toml"))
    has_datus_dir = os.path.exists(os.path.join(project_root, "datus"))
    return has_pyproject and has_datus_dir
```

### 2. å¤šç­–ç•¥è§£æ

**DDL è§£æå¤šç­–ç•¥å›é€€ï¼š**
1. ä¸»è¦ç­–ç•¥ï¼šsqlglot è§£æ
2. å›é€€ç­–ç•¥ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼è§£æ
3. æ¸…ç†ç­–ç•¥ï¼šç§»é™¤é”™è¯¯æ¶ˆæ¯ç‰‡æ®µ
4. ä¸­æ–‡ä¿æŠ¤ï¼šä¿ç•™ä¸­æ–‡æ³¨é‡Š

### 3. çº¿ç¨‹å®‰å…¨

**è·¯å¾„ç®¡ç†å™¨åŒæ£€é”ï¼š**
```python
def get_path_manager() -> DatusPathManager:
    global _path_manager
    if _path_manager is None:
        with _path_manager_lock:
            if _path_manager is None:
                _path_manager = DatusPathManager()
    return _path_manager
```

### 4. ç±»å‹å®‰å…¨

**SQL ç±»å‹å®‰å…¨æ£€æŸ¥ï¼š**
- `ALLOWED_SQL_TYPES` ç™½åå•æœºåˆ¶
- è¾“å…¥éªŒè¯ï¼ˆé•¿åº¦ã€ç±»å‹ã€æ‹¬å·å¹³è¡¡ï¼‰
- æ–¹è¨€æ˜ å°„å’Œè§„èŒƒåŒ–

### 5. å¯æ‰©å±•æ€§

**è£…é¥°å™¨é©±åŠ¨ï¼š**
```python
@unified_error_handler("ExecuteSQLNode", "sql_execution")
def execute(self, input_data: ExecuteSQLInput) -> ExecuteSQLResult:
    # è‡ªåŠ¨é”™è¯¯å¤„ç†
    pass

@optional_traceable(name="custom_name", run_type="chain")
def custom_function():
    # è‡ªåŠ¨ LangSmith è¿½è¸ª
    pass
```

---

## ä½¿ç”¨ç¤ºä¾‹

### SQL DDL è§£æ

```python
from datus.utils.sql_utils import parse_metadata_from_ddl

ddl = """
CREATE TABLE users (
    id INT PRIMARY KEY COMMENT 'ç”¨æˆ·ID',
    name VARCHAR(100) COMMENT 'ç”¨æˆ·å',
    created_at TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´'
) COMMENT='ç”¨æˆ·è¡¨'
"""

result = parse_metadata_from_ddl(ddl, dialect=DBType.STARROCKS)
# è¿”å›:
# {
#     "table": {"name": "users", "comment": "ç”¨æˆ·è¡¨"},
#     "columns": [
#         {"name": "id", "type": "INT", "comment": "ç”¨æˆ·ID"},
#         {"name": "name", "type": "VARCHAR(100)", "comment": "ç”¨æˆ·å"},
#         {"name": "created_at", "type": "TIMESTAMP", "comment": "åˆ›å»ºæ—¶é—´"}
#     ],
#     "primary_keys": ["id"]
# }
```

### LLM ç»“æœè§£æ

```python
from datus.utils.json_utils import llm_result2json

llm_output = '''
Here's the result:
```json
{
    "sql": "SELECT * FROM users",
    "explanation": "Get all users"
}
```
'''

result = llm_result2json(llm_output)
# è¿”å›: {"sql": "SELECT * FROM users", "explanation": "Get all users"}
```

### è·¯å¾„ç®¡ç†

```python
from datus.utils.path_manager import get_path_manager

pm = get_path_manager()

# è·å–å„ç§è·¯å¾„
config_path = pm.agent_config_path()        # ~/.datus/conf/agent.yml
logs_dir = pm.logs_dir                    # ~/.datus/logs
rag_path = pm.rag_storage_path("default") # ~/.datus/data/datus_db_default

# åˆ›å»ºä¿å­˜ç›®å½•
save_dir = pm.save_run_dir("namespace", "run_id")
# è‡ªåŠ¨åˆ›å»ºç›®å½•å¹¶è¿”å›è·¯å¾„
```

### æ•°æ®å‹ç¼©

```python
from datus.utils.compress_utils import DataCompressor

compressor = DataCompressor(
    model_name="gpt-4o",
    token_threshold=2000,
    output_format="csv"
)

# å‹ç¼©å¤§å‹æŸ¥è¯¢ç»“æœ
result = compressor.compress(large_dataframe)
# è¿”å›:
# {
#     "original_rows": 10000,
#     "is_compressed": True,
#     "compressed_data": "...",  # å‰10è¡Œ + ... + å10è¡Œ
#     "compression_type": "rows"
# }
```

### å¼‚æ­¥æ‰§è¡Œ

```python
from datus.utils.async_utils import run_async

async def async_function():
    await asyncio.sleep(1)
    return "async result"

# åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
result = run_async(async_function())
```

### é”™è¯¯å¤„ç†

```python
from datus.utils.exceptions import DatusException, ErrorCode
from datus.utils.error_handling import unified_error_handler

@unified_error_handler("MyNode", "my_operation")
def my_operation():
    # è‡ªåŠ¨é”™è¯¯å¤„ç†
    raise DatusException(
        ErrorCode.DB_CONNECTION_FAILED,
        message="Database connection timeout"
    )
```

---

## ç‰ˆæœ¬æ›´æ–°è®°å½•

### v2.0 (2026-01-22)
- å®Œæ•´é‡å†™ï¼ŒåŸºäºæœ€æ–°ä»£ç æ¶æ„
- æ–°å¢ 28 ä¸ªå·¥å…·æ¨¡å—è¯¦ç»†è¯´æ˜
- æ–°å¢ ErrorCode 7 ä½é”™è¯¯ç ä½“ç³»
- æ–°å¢ DatusException æ ‡å‡†åŒ–å¼‚å¸¸
- æ–°å¢ DynamicLogManager åŠ¨æ€æ—¥å¿—ç³»ç»Ÿ
- æ–°å¢ DatusPathManager é›†ä¸­å¼è·¯å¾„ç®¡ç†
- æ–°å¢ SubAgentManager Sub-Agent ç®¡ç†
- æ–°å¢ DataCompressor æ•°æ®å‹ç¼©å·¥å…·
- æ–°å¢ SQL å·¥å…·å®Œæ•´æ–‡æ¡£ï¼ˆDDL è§£æã€ä¸­æ–‡æ³¨é‡Šä¿æŠ¤ï¼‰
- æ–°å¢ JSON å·¥å…·å®Œæ•´æ–‡æ¡£ï¼ˆLLM ç»“æœè§£æï¼‰
- æ–°å¢ç»Ÿä¸€é”™è¯¯å¤„ç†è£…é¥°å™¨
- æ–°å¢å¯è¿½è¸ªè£…é¥°å™¨æ”¯æŒ
- å®Œå–„å¼‚æ­¥è¿è¡Œæ—¶æ–‡æ¡£

### v1.0 (2026-01-05)
- åˆå§‹ç‰ˆæœ¬
- é«˜å±‚æ¬¡æ¶æ„æ¦‚è¿°
