# Datus 5 å¤§åœºæ™¯æ”¹è¿›æ–¹æ¡ˆ

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
> **æ›´æ–°æ—¥æœŸ**: 2026-01-17

---

## æ–‡æ¡£æ¦‚è¿°

åŸºäºå¯¹ Datus æœ€æ–°ä»£ç åº“ï¼ˆ`/Users/lonlyhuang/workspace/git/Datus-agent/datus`ï¼‰çš„æ·±å…¥åˆ†æï¼Œç»“åˆä¹‹å‰å®Œæˆçš„èŠ‚ç‚¹æ¨¡å—æ–‡æ¡£ï¼Œæœ¬æ–‡æ¡£æä¾›é’ˆå¯¹5å¤§ä¸šåŠ¡åœºæ™¯çš„è¯¦ç»†æ”¹è¿›æ–¹æ¡ˆå’Œå®æ–½è·¯çº¿å›¾ã€‚

**æ ¸å¿ƒå‘ç°:**
- **æ™ºèƒ½é—®æ•°**: âœ… å®Œæ•´å®ç°ï¼ˆä¸¤é˜¶æ®µæ„å›¾å¤„ç† + 9æ­¥Text2SQLå·¥ä½œæµï¼‰
- **SQLç”Ÿæˆ**: âœ… å®Œæ•´å®ç°ï¼ˆå¤šç§ç­–ç•¥ + åå°„æœºåˆ¶ï¼‰
- **SQLå®¡æŸ¥**: âœ… å®Œæ•´å®ç°ï¼ˆ7ç§é¢„æ£€å·¥å…· + 3ç§å¢å¼ºå·¥å…·ï¼‰
- **æ·±åº¦åˆ†æ**: âš ï¸ éƒ¨åˆ†å®ç°ï¼ˆç¼ºå°‘ä¸“ç”¨å·¥ä½œæµå’Œé«˜çº§åˆ†ææ¨¡å¼ï¼‰
- **æ•°æ®è´¨æ£€**: âŒ æœ‰é™å®ç°ï¼ˆä»…åŸºç¡€ç»“æœéªŒè¯ï¼Œç¼ºå°‘å…¨é¢è´¨æ£€èƒ½åŠ›ï¼‰

**æœ€æ–°ä»£ç å˜åŒ– (2026-01-17):**
- âœ… IntentAnalysisNodeï¼ˆå¯å‘å¼æ„å›¾æ£€æµ‹ + LLMå›é€€ï¼‰
- âœ… IntentClarificationNodeï¼ˆçº é”™ + æ¶ˆæ­§ + å®ä½“æå–ï¼‰
- âœ… EnhancedPreflightToolsï¼ˆquery_planåˆ†æ + å†²çªæ£€æµ‹ + åˆ†åŒºéªŒè¯ï¼‰
- âœ… SQLé¢„éªŒè¯ï¼ˆvalidate_and_suggest_sql_fixesï¼‰
- âœ… DDL/DML safeguardsï¼ˆå®‰å…¨é˜²æŠ¤ï¼‰
- âœ… **SchemaDiscoveryNodeå¢å¼º** - é›†æˆSchemaLinkingNodeèƒ½åŠ›ï¼ˆæ¸è¿›å¼åŒ¹é…ã€å¤–éƒ¨çŸ¥è¯†å¢å¼ºã€LLM SchemaåŒ¹é…ï¼‰

**ğŸš¨ P0 ä¼˜åŒ–ä»»åŠ¡ (è¿›è¡Œä¸­):**
- âš ï¸ **SchemaLinkingNode å’Œ SchemaDiscoveryNode ç»Ÿä¸€è¿ç§»** - Phase 0.1 å·²å®Œæˆ
- ğŸ“‹ **å·²å®Œæˆä»£ç å®ç°** - feature/schema-discovery-enhancement åˆ†æ”¯
- ğŸ“‹ **å¾…å®Œæˆ**: workflow.yml æ›´æ–°ã€æµ‹è¯•å¥—ä»¶ã€ä¸»åˆ†æ”¯åˆå¹¶

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šåœºæ™¯ç°çŠ¶åˆ†æ

### åœºæ™¯1: æ™ºèƒ½é—®æ•° âœ… å®Œæ•´å®ç°

**ç›®æ ‡**: æ ¹æ®ä¸šåŠ¡åˆ†æéœ€æ±‚æä¾›æŸ¥è¯¢SQL/æ•°æ®/æ•°æ®è§£è¯»

**å½“å‰å®ç°**:
```python
# Text2SQL å·¥ä½œæµ (9æ­¥å®Œæ•´æµç¨‹)
text2sql:
  - intent_analysis       # å¯å‘å¼æ„å›¾æ£€æµ‹ (å…³é”®è¯ + LLMå›é€€)
  - intent_clarification  # LLMæ„å›¾æ¾„æ¸… (çº é”™ + æ¶ˆæ­§ + å®ä½“æå–)
  - schema_discovery      # è¯­ä¹‰æœç´¢ + å…³é”®è¯åŒ¹é… + LLMæ¨ç†
  - schema_validation     # æ¨¡å¼éªŒè¯
  - generate_sql          # SQLç”Ÿæˆ (llm_result2jsonæ ‡å‡†åŒ–)
  - sql_validate          # è¯­æ³•éªŒè¯ + æ¨¡å¼éªŒè¯ + DDL/DML safeguards
  - execute_sql           # å¼‚æ­¥æ‰§è¡Œ + è¿æ¥æ± 
  - result_validation     # ç»“æœè´¨é‡éªŒè¯
  - reflect               # 4ç­–ç•¥åæ€æœºåˆ¶
  - output                # ç»“æœè¾“å‡º
```

**å…³é”®æ–‡ä»¶**:
- `datus/agent/node/intent_analysis_node.py` - å¯å‘å¼æ„å›¾æ£€æµ‹
- `datus/agent/node/intent_clarification_node.py` - LLMæ„å›¾æ¾„æ¸…
- `datus/agent/node/schema_discovery_node.py` - æ¨¡å¼å‘ç°
- `datus/agent/node/generate_sql_node.py` - SQLç”Ÿæˆ
- `datus/agent/node/sql_validate_node.py` - SQLéªŒè¯
- `datus/agent/node/execute_sql_node.py` - SQLæ‰§è¡Œ
- `datus/agent/node/reflect_node.py` - åæ€æœºåˆ¶

**èƒ½åŠ›äº®ç‚¹**:
1. **ä¸¤é˜¶æ®µæ„å›¾å¤„ç†**:
   - IntentAnalysisNode: å¿«é€Ÿå¯å‘å¼æ£€æµ‹ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰+ LLMå›é€€ï¼ˆconfidence < 0.7ï¼‰
   - IntentClarificationNode: çº é”™ï¼ˆ"åå±±" â†’ "åå—"ï¼‰+ æ¶ˆæ­§ + å®ä½“æå–

2. **æ™ºèƒ½æ¨¡å¼å‘ç°**:
   - è¯­ä¹‰æœç´¢ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰
   - å…³é”®è¯åŒ¹é…
   - LLMæ¨ç†

3. **æ ‡å‡†åŒ–JSONè§£æ**:
   - `llm_result2json()` ç»Ÿä¸€å¤„ç†æ‰€æœ‰LLMå“åº”
   - æ”¯æŒmarkdownã€æˆªæ–­JSONã€æ ¼å¼é”™è¯¯ä¿®å¤

4. **SQLé¢„éªŒè¯**:
   - `validate_and_suggest_sql_fixes()` è¯­æ³•éªŒè¯
   - DDL/DML safeguards é˜²æ­¢æœªæˆæƒschemaä¿®æ”¹

**å¢å¼ºå»ºè®®**:
1. DataInterpretationNode - æ•°æ®æ´å¯Ÿç”Ÿæˆ
2. ConversationMemoryNode - é•¿æœŸå¯¹è¯è®°å¿†
3. å¤šè½®å¯¹è¯ä¼˜åŒ–

---

### åœºæ™¯2: SQLç”Ÿæˆ âœ… å®Œæ•´å®ç°

**ç›®æ ‡**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚ç”Ÿæˆæ»¡è¶³è§„èŒƒçš„é«˜æ€§èƒ½SQL

**å½“å‰å®ç°**:
```python
# å¤šç§SQLç”Ÿæˆç­–ç•¥
1. Text2SQL å·¥ä½œæµ (9æ­¥)
2. metric_to_sql å·¥ä½œæµ (æŒ‡æ ‡é©±åŠ¨)
3. gensql_agentic å·¥ä½œæµ (ä¼šè¯å¼ç”Ÿæˆ)
4. reflection å·¥ä½œæµ (åæ€å¼ç”Ÿæˆ)
```

**å…³é”®ç‰¹æ€§**:
- `llm_result2json()` æ ‡å‡†åŒ–JSONè§£æ
- 1å°æ—¶TTLç¼“å­˜
- å¤šç§ç”Ÿæˆç­–ç•¥ (plan-based, schema-based, direct)
- åå°„æœºåˆ¶ (4ç§ç­–ç•¥)

**åå°„ç­–ç•¥è¯¦è§£**:
```python
# ReflectNode æ”¯æŒçš„4ç§ç­–ç•¥
strategies = {
    "schema_linking": {"max_iterations": 2, "focus": "è¡¨é“¾æ¥ä¼˜åŒ–"},
    "simple_regenerate": {"max_iterations": 3, "focus": "ç®€å•é‡æ–°ç”Ÿæˆ"},
    "reasoning": {"max_iterations": 3, "focus": "æ¨ç†å¼ç”Ÿæˆ"},
    "doc_search": {"max_iterations": 1, "focus": "æ–‡æ¡£æœç´¢"}
}
```

**ä¼˜åŒ–å»ºè®®**:
1. PerformanceOptimizationNode - SQLæ€§èƒ½åˆ†æ
2. SQLRewriteNode - ç­‰ä»·SQLé‡å†™
3. ExecutionPlanAnalysisNode - æ·±åº¦æ‰§è¡Œè®¡åˆ’åˆ†æ

---

### åœºæ™¯3: SQLå®¡æŸ¥ âœ… å®Œæ•´å®ç°

**ç›®æ ‡**: ä»å¤šç»´åº¦å®¡æŸ¥SQLçš„æ­£ç¡®æ€§ã€åˆç†æ€§ã€è§„èŒƒæ€§

**å½“å‰å®ç°**:
```python
# SQLå®¡æŸ¥é¢„æ£€ç¼–æ’å™¨ (7ç§å·¥å…·)
Legacy Tools:
  - describe_table           # è¡¨ç»“æ„æè¿°
  - search_external_knowledge  # å¤–éƒ¨çŸ¥è¯†æ£€ç´¢
  - read_query               # SQLæŸ¥è¯¢è¯»å–
  - get_table_ddl            # DDLè·å–

Enhanced Tools (v2.4):
  - analyze_query_plan       # æ‰§è¡Œè®¡åˆ’åˆ†æ (with fallback)
  - check_table_conflicts    # è¡¨å†²çªæ£€æµ‹
  - validate_partitioning    # åˆ†åŒºéªŒè¯
```

**å…³é”®æ–‡ä»¶**:
- `datus/agent/node/sql_review_preflight_orchestrator.py` - SQLå®¡æŸ¥é¢„æ£€ç¼–æ’
- `datus/tools/func_tool/enhanced_preflight_tools.py` - å¢å¼ºé¢„æ£€å·¥å…·
- `datus/agent/node/chat_agentic_node.py` - å¯¹è¯å¼å®¡æŸ¥ (sql_reviewæ¨¡å¼)

**å¢å¼ºç‰¹æ€§**:
1. **SQLé¢„éªŒè¯**: `validate_and_suggest_sql_fixes()` è¯­æ³•éªŒè¯ + ä¿®å¤å»ºè®®
2. **å…³é”®å·¥å…· vs è¾…åŠ©å·¥å…·åˆ†ç±»**:
   - å…³é”®å·¥å…·: `describe_table`, `search_external_knowledge`ï¼ˆå¿…é¡»æˆåŠŸï¼‰
   - è¾…åŠ©å·¥å…·: `read_query`, `get_table_ddl`, `analyze_query_plan`, `check_table_conflicts`, `validate_partitioning`ï¼ˆå¯é€‰ï¼‰
3. **Fallbackè§„åˆ™åˆ†æ**: å½“EXPLAINå¤±è´¥æ—¶ï¼Œä½¿ç”¨é™æ€è§„åˆ™åˆ†æSQLæ€§èƒ½
4. **åˆ†åŒºç­–ç•¥éªŒè¯**: æ£€æµ‹åˆ†åŒºé”®é€‰æ‹©ã€åˆ†åŒºç±»å‹ã€æ—¶é—´åˆ†åŒºå»ºè®®
5. **è¡¨å†²çªæ£€æµ‹**: ç›¸ä¼¼è¡¨ç»“æ„æ£€æµ‹ã€åˆ†å±‚è¿è§„æ£€æµ‹ã€é‡å¤å»ºè®¾é£é™©è¯„ä¼°

**Fallbackè§„åˆ™åˆ†æç¤ºä¾‹**:
```python
# enhanced_preflight_tools.py ä¸­çš„fallbackåˆ†æ
hotspots = []
if "SELECT *" in sql:
    hotspots.append({
        "reason": "select_star",
        "severity": "medium",
        "recommendation": "Specify only needed columns"
    })
if "LIKE '%...%'" in sql:
    hotspots.append({
        "reason": "leading_wildcard_like",
        "severity": "medium",
        "recommendation": "Leading wildcards prevent index usage"
    })
```

**ä¼˜åŒ–å»ºè®®**:
1. BusinessRuleValidationNode - ä¸šåŠ¡è§„åˆ™éªŒè¯
2. SecurityAuditNode - SQLå®‰å…¨å®¡è®¡
3. BestPracticeCheckNode - æœ€ä½³å®è·µæ£€æŸ¥
4. ReviewReportNode - å®¡æŸ¥æŠ¥å‘Šç”Ÿæˆ

---

### åœºæ™¯4: æ·±åº¦åˆ†æ âš ï¸ éƒ¨åˆ†å®ç°

**ç›®æ ‡**: æ¢ç´¢å¼æ•°æ®åˆ†æï¼Œç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š

**å½“å‰å®ç°**:
```python
# ç°æœ‰æ·±åº¦åˆ†æèƒ½åŠ›
- ChatAgenticNode (data_analysisæ¨¡å¼)
- ExecuteSQLNode (æ•°æ®æŸ¥è¯¢)
- ReflectNode (æœ‰é™åæ€)
- PreflightOrchestrator (æ•°æ®å‡†å¤‡)
```

**èƒ½åŠ›å·®è·**:
1. âŒ æ¢ç´¢å¼åˆ†æå·¥ä½œæµç¼ºå¤±
2. âŒ ReActæ¨ç†å¾ªç¯ç¼ºå¤±
3. âŒ ç»Ÿè®¡åˆ†æèƒ½åŠ›ç¼ºå¤±
4. âŒ å‡è®¾ç”Ÿæˆå’ŒéªŒè¯ç¼ºå¤±
5. âŒ å¯è§†åŒ–ç”Ÿæˆç¼ºå¤±
6. âŒ æŠ¥å‘Šç”Ÿæˆç¼ºå¤±

**æ–°å¢å·¥ä½œæµè®¾è®¡**:
```yaml
# workflow.yml æ–°å¢
deep_analysis:
  - intent_analysis
  - intent_clarification
  - exploratory_analysis    # æ–°å¢ï¼šæ¢ç´¢å¼åˆ†æ
  - statistical_analysis    # æ–°å¢ï¼šç»Ÿè®¡åˆ†æ
  - hypothesis_testing      # æ–°å¢ï¼šå‡è®¾éªŒè¯
  - visualization           # æ–°å¢ï¼šå¯è§†åŒ–ç”Ÿæˆ
  - report_generation       # æ–°å¢ï¼šæŠ¥å‘Šç”Ÿæˆ
  - output
```

---

### åœºæ™¯5: æ•°æ®è´¨æ£€ âŒ æœ‰é™å®ç°

**ç›®æ ‡**: åˆ†æè¡¨çš„å»ºè¡¨è§„èŒƒã€å­—æ®µè§„èŒƒã€ç´¢å¼•ä½¿ç”¨ã€æ•°æ®è´¨é‡

**å½“å‰å®ç°**:
```python
# ç°æœ‰è´¨æ£€èƒ½åŠ›
- ResultValidationNode (åŸºç¡€ç»“æœéªŒè¯)
- SQLValidateNode (SQLè¯­æ³•éªŒè¯)
- EnhancedPreflightTools (éƒ¨åˆ†è´¨é‡æ£€æŸ¥)
```

**èƒ½åŠ›å·®è·**:
1. âŒ å»ºè¡¨è§„èŒƒæ£€æŸ¥ç¼ºå¤±
2. âŒ å­—æ®µè§„èŒƒæ£€æŸ¥ç¼ºå¤±
3. âŒ ç´¢å¼•ä½¿ç”¨åˆ†æç¼ºå¤±
4. âŒ æ•°æ®è´¨é‡æ£€æŸ¥ç¼ºå¤±ï¼ˆå®Œæ•´æ€§ã€ä¸€è‡´æ€§ã€å‡†ç¡®æ€§ï¼‰
5. âŒ æ•°æ®æ¼‚ç§»æ£€æµ‹ç¼ºå¤±
6. âŒ è´¨æ£€æŠ¥å‘Šç”Ÿæˆç¼ºå¤±

**æ–°å¢å·¥ä½œæµè®¾è®¡**:
```yaml
# workflow.yml æ–°å¢
data_quality:
  - schema_standards_check   # æ–°å¢ï¼šè§„èŒƒæ£€æŸ¥
  - data_profiling           # æ–°å¢ï¼šæ•°æ®ç”»åƒ
  - data_quality_check       # æ–°å¢ï¼šè´¨é‡æ£€æŸ¥
  - index_analysis           # æ–°å¢ï¼šç´¢å¼•åˆ†æ
  - quality_report           # æ–°å¢ï¼šè´¨æ£€æŠ¥å‘Š
  - output
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šæœ€æ–°ä»£ç å®ç°åˆ†æ

### IntentAnalysisNode å®ç°

**æ–‡ä»¶**: `datus/agent/node/intent_analysis_node.py`

**æ ¸å¿ƒåŠŸèƒ½**:
```python
class IntentAnalysisNode(Node):
    """
    å¯å‘å¼æ„å›¾æ£€æµ‹èŠ‚ç‚¹

    åŠŸèƒ½:
    - å…³é”®è¯æ£€æµ‹ï¼ˆå¿«é€Ÿï¼‰
    - LLM å›é€€æœºåˆ¶ï¼ˆconfidence < 0.7ï¼‰
    - è·³è¿‡é€»è¾‘ï¼ˆå½“ execution_mode é¢„è®¾æ—¶ï¼‰
    """

    async def _detect_intent(self, task_text: str) -> IntentResult:
        # 1. å¯å‘å¼æ£€æµ‹ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        heuristic_result = detector.detect_sql_intent_by_keyword(task_text)
        is_sql_intent, metadata = heuristic_result

        # 2. è®¡ç®—confidence
        confidence = min(total_matches * 0.2, 0.8)
        if has_patterns:
            confidence += 0.2

        # 3. LLMå›é€€ï¼ˆå¦‚æœconfidence < 0.7ï¼‰
        if confidence < 0.7 and use_llm_fallback:
            llm_result = await detector.classify_intent_with_llm(task_text, model)
```

**è¾“å‡º**:
- `workflow.metadata["detected_intent"]` - æ£€æµ‹åˆ°çš„æ„å›¾ç±»å‹
- `workflow.metadata["intent_confidence"]` - ç½®ä¿¡åº¦ (0-1)
- `workflow.metadata["intent_metadata"]` - å…ƒæ•°æ®ï¼ˆåŒ¹é…çš„å…³é”®è¯ã€æ¨¡å¼ç­‰ï¼‰

---

### IntentClarificationNode å®ç°

**æ–‡ä»¶**: `datus/agent/node/intent_clarification_node.py`

**æ ¸å¿ƒåŠŸèƒ½**:
```python
class IntentClarificationNode(Node, LLMMixin):
    """
    ä¸šåŠ¡æ„å›¾æ¾„æ¸…èŠ‚ç‚¹

    åŠŸèƒ½:
    - çº é”™ (e.g., "åå±±" â†’ "åå—")
    - æ¶ˆæ­§ (e.g., "æœ€è¿‘çš„é”€å”®" â†’ "æœ€è¿‘30å¤©çš„é”€å”®æ•°æ®")
    - å®ä½“æå– (business_terms, time_range, dimensions, metrics)
    """

    async def _clarify_intent(self, task_text: str, ext_knowledge: str) -> Dict:
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾...

        è¾“å‡ºJSONæ ¼å¼ï¼š
        {
            "clarified_task": "æ¾„æ¸…å’Œè§„èŒƒåŒ–åçš„æŸ¥è¯¢",
            "entities": {
                "business_terms": ["ä¸šåŠ¡æœ¯è¯­"],
                "time_range": "æ—¶é—´èŒƒå›´",
                "dimensions": ["æ•°æ®ç»´åº¦"],
                "metrics": ["æŒ‡æ ‡åç§°"]
            },
            "corrections": {
                "typos_fixed": ["çº æ­£çš„é”™åˆ«å­—"],
                "ambiguities_resolved": ["æ¾„æ¸…çš„æ¨¡ç³Šè¡¨è¿°"]
            },
            "confidence": 0.95
        }"""

        # ä½¿ç”¨ llm_call_with_retry å’Œ1å°æ—¶TTLç¼“å­˜
        response = await self.llm_call_with_retry(
            prompt=prompt,
            operation_name="intent_clarification",
            cache_key=f"intent_clarification:{hash(task_text)}",
            max_retries=3
        )

        # ä½¿ç”¨ llm_result2json æ ‡å‡†åŒ–è§£æ
        clarification_result = llm_result2json(response_text, expected_type=dict)
```

**è¾“å‡º**:
- `workflow.metadata["clarified_task"]` - æ¾„æ¸…åçš„ä»»åŠ¡
- `workflow.metadata["original_task"]` - åŸå§‹ä»»åŠ¡
- `workflow.metadata["intent_clarification"]` - å®Œæ•´æ¾„æ¸…ç»“æœ

---

### EnhancedPreflightTools å®ç°

**æ–‡ä»¶**: `datus/tools/func_tool/enhanced_preflight_tools.py`

**ä¸‰ä¸ªå¢å¼ºå·¥å…·**:

#### 1. analyze_query_plan - æ‰§è¡Œè®¡åˆ’åˆ†æ
```python
async def analyze_query_plan(self, sql: str, catalog: str, database: str, schema: str):
    # 1. æ‰§è¡Œ EXPLAIN æŸ¥è¯¢
    explain_sql = f"EXPLAIN {sql}"
    result = connector.execute_arrow(explain_sql)

    # 2. åˆ†ææ‰§è¡Œè®¡åˆ’
    analysis = self._analyze_plan_text(plan_text, db_type)

    # 3. è¿”å›åˆ†æç»“æœ
    return {
        "estimated_cost": analysis["estimated_cost"],
        "estimated_rows": analysis["estimated_rows"],
        "hotspots": analysis["hotspots"],  # æ€§èƒ½çƒ­ç‚¹
        "join_analysis": analysis["join_analysis"],
        "index_usage": analysis["index_usage"],
        "recommendations": analysis["recommendations"]
    }
```

**Fallbackåˆ†æ** (å½“EXPLAINå¤±è´¥æ—¶):
```python
def _fallback_query_analysis(self, sql: str, error: str):
    hotspots = []
    # è§„åˆ™1: SELECT * æ£€æµ‹
    if re.search(r'SELECT\s+\*\s+FROM', sql):
        hotspots.append({"reason": "select_star", "severity": "medium"})

    # è§„åˆ™2: LIKE '%...%' æ£€æµ‹
    if re.search(r'LIKE\s+[\'"]?%\w+%[\'"]?', sql):
        hotspots.append({"reason": "leading_wildcard_like", "severity": "medium"})

    # è§„åˆ™3: å‡½æ•°ç´¢å¼•åˆ—æ£€æµ‹
    # è§„åˆ™4: JOINæ— ONæ¡ä»¶æ£€æµ‹
    # è§„åˆ™5: ORDER BYæ— LIMITæ£€æµ‹
```

#### 2. check_table_conflicts - è¡¨å†²çªæ£€æµ‹
```python
async def check_table_conflicts(self, table_name: str, catalog: str, database: str, schema: str):
    # 1. è·å–è¡¨ä¿¡æ¯
    table_info = self.schema_rag.get_table_schema(table_name, catalog, database, schema)

    # 2. æŸ¥æ‰¾ç›¸ä¼¼è¡¨ç»“æ„
    similar_tables = self._find_similar_tables(table_info, catalog, database, schema)

    # 3. åˆ†æåˆ†å±‚è¿è§„
    layering_violations = self._analyze_layering_violations(table_name, similar_tables)

    # 4. è¯„ä¼°é‡å¤å»ºè®¾é£é™©
    duplicate_risk = self._assess_duplicate_risk(similar_tables, layering_violations)

    return {
        "exists_similar": len(similar_tables) > 0,
        "matches": similar_tables,  # ç›¸ä¼¼åº¦ > 60% çš„è¡¨
        "duplicate_build_risk": duplicate_risk,  # high/medium/low
        "layering_violations": layering_violations,
        "recommendations": self._generate_conflict_recommendations(...)
    }
```

**åˆ†å±‚è¿è§„æ£€æµ‹**:
```python
def _analyze_layering_violations(self, table_name: str, similar_tables: List[Dict]):
    violations = []
    # æ£€æµ‹åˆ†å±‚æ¨¡å¼
    ods_patterns = ["ods_", "origin_", "raw_"]
    dwd_patterns = ["dwd_", "dim_", "fact_"]
    dws_patterns = ["dws_", "summary_", "agg_"]
    ads_patterns = ["ads_", "report_", "dashboard_"]

    # æ£€æŸ¥è¡¨åæ˜¯å¦åŒæ—¶åŒ¹é…å¤šä¸ªåˆ†å±‚æ¨¡å¼
    detected_layers = []
    for layer_name, patterns in layer_patterns:
        if any(pattern in table_lower for pattern in patterns):
            detected_layers.append(layer_name)

    if len(detected_layers) > 1:
        violations.append(f"è¡¨åæš—ç¤ºå¤šä¸ªåˆ†å±‚: {detected_layers}")
```

#### 3. validate_partitioning - åˆ†åŒºéªŒè¯
```python
async def validate_partitioning(self, table_name: str, catalog: str, database: str, schema: str):
    # 1. è·å–è¡¨DDL
    ddl_result = connector.get_table_ddl(table_name, catalog, database, schema)

    # 2. è§£æåˆ†åŒºä¿¡æ¯
    partition_info = self._parse_partition_info(ddl_text)
    # ç¤ºä¾‹: {"is_partitioned": True, "partition_type": "RANGE", "partition_key": "date_column"}

    # 3. éªŒè¯åˆ†åŒºç­–ç•¥
    validation_results = self._validate_partition_strategy(partition_info, table_name)

    # 4. ç”Ÿæˆåˆ†åŒºå»ºè®®
    recommendations = self._generate_partition_recommendations(partition_info, validation_results)

    return {
        "partitioned": partition_info.get("is_partitioned", False),
        "partition_info": partition_info,
        "validation_results": validation_results,  # {"is_valid": bool, "score": 0-100}
        "issues": validation_results.get("issues", []),
        "recommended_partition": recommendations,
        "performance_impact": performance_impact
    }
```

**åˆ†åŒºç­–ç•¥éªŒè¯**:
```python
def _validate_partition_strategy(self, partition_info: Dict, table_name: str):
    validation_results = {"is_valid": True, "issues": [], "warnings": [], "score": 100}

    if not partition_info.get("is_partitioned"):
        # å¤§è¡¨åº”è€ƒè™‘åˆ†åŒº
        if any(keyword in table_lower for keyword in ["fact", "log", "event", "metric"]):
            validation_results["warnings"].append("å¤§è¡¨åº”è€ƒè™‘åˆ†åŒºä»¥æå‡æ€§èƒ½")
            validation_results["score"] -= 20
        return validation_results

    # æ£€æŸ¥åˆ†åŒºé”®è´¨é‡
    partition_key = partition_info.get("partition_key")
    time_indicators = ["date", "time", "timestamp", "created_at", "updated_at"]
    has_time_key = any(indicator in partition_key.lower() for indicator in time_indicators)

    if not has_time_key:
        validation_results["warnings"].append("å»ºè®®ä½¿ç”¨æ—¶é—´å‹åˆ†åŒºé”®")
        validation_results["score"] -= 15

    # é¿å…é«˜åŸºæ•°é”®
    if any(indicator in partition_key.lower() for indicator in ["id", "uuid", "hash"]):
        validation_results["warnings"].append("é«˜åŸºæ•°åˆ†åŒºé”®å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜")
        validation_results["score"] -= 10
```

---

### Text2SQL å·¥ä½œæµå®ç°

**æ–‡ä»¶**: `datus/agent/workflow.yml`

**å®Œæ•´æµç¨‹**:
```yaml
text2sql:
  - intent_analysis       # Step 1: ä»»åŠ¡ç±»å‹æ£€æµ‹ (text2sql/sql_review/data_analysis)
  - intent_clarification  # Step 2: ä¸šåŠ¡æ„å›¾æ¾„æ¸… (çº é”™+æ¶ˆæ­§+å®ä½“æå–)
  - schema_discovery      # Step 3: æ¨¡å¼å‘ç° (è¯­ä¹‰+å…³é”®è¯+LLM)
  - schema_validation     # Step 4: æ¨¡å¼éªŒè¯ (åˆ—å­˜åœ¨æ€§+æ¨¡ç³ŠåŒ¹é…)
  - generate_sql          # Step 5: SQLç”Ÿæˆ (llm_result2jsonæ ‡å‡†åŒ–)
  - sql_validate          # Step 6: SQLéªŒè¯ (è¯­æ³•+æ¨¡å¼+DDL/DML safeguards)
  - execute_sql           # Step 7: SQLæ‰§è¡Œ (å¼‚æ­¥+è¿æ¥æ± )
  - result_validation     # Step 8: ç»“æœéªŒè¯ (è´¨é‡æ£€æŸ¥)
  - reflect               # Step 9: åæ€æœºåˆ¶ (4ç­–ç•¥è‡ªçº é”™)
  - output                # Step 10: ç»“æœè¾“å‡º
```

**å…³é”®æ•°æ®æµ**:
```
IntentAnalysisNode
  â†“ workflow.metadata["detected_intent"]
IntentClarificationNode
  â†“ workflow.metadata["clarified_task"]
SchemaDiscoveryNode (ä½¿ç”¨clarified_taskè¿›è¡Œè¯­ä¹‰æœç´¢)
  â†“ workflow.context.table_schemas
SchemaValidationNode (éªŒè¯schemaå……åˆ†æ€§)
  â†“ éªŒè¯é€šè¿‡çš„schema
GenerateSQLNode (ç”ŸæˆSQL)
  â†“ workflow.context.generated_sql
SQLValidateNode (éªŒè¯SQL)
  â†“ éªŒè¯é€šè¿‡çš„SQL
ExecuteSQLNode (æ‰§è¡ŒSQL)
  â†“ SQLæ‰§è¡Œç»“æœ
ResultValidationNode (éªŒè¯ç»“æœè´¨é‡)
  â†“ éªŒè¯é€šè¿‡çš„ç»“æœ
ReflectNode (åæ€å’Œè‡ªçº é”™)
  â†“ å¯èƒ½é‡æ–°ç”ŸæˆSQL
OutputNode (è¾“å‡ºæœ€ç»ˆç»“æœ)
```

---

### å®‰å…¨é˜²æŠ¤å®ç°

**DDL/DML Safeguards**:

```python
# sql_validate_node.py ä¸­çš„å®‰å…¨æ£€æŸ¥
def _validate_ddl_dml_safety(self, sql: str, database_type: str):
    """
    éªŒè¯DDL/DMLæ“ä½œçš„å®‰å…¨æ€§

    æ£€æŸ¥é¡¹:
    1. DDLæ“ä½œ (CREATE/ALTER/DROP TABLE) - éœ€è¦æ˜¾å¼æˆæƒ
    2. DMLæ“ä½œ (UPDATE/DELETE) - éœ€è¦WHEREæ¡ä»¶
    3. å±é™©å‡½æ•° (DROP DATABASE, TRUNCATE) - ç¦æ­¢
    """
    sql_upper = sql.strip().upper()

    # 1. æ£€æµ‹DDLæ“ä½œ
    ddl_keywords = ["CREATE TABLE", "ALTER TABLE", "DROP TABLE"]
    has_ddl = any(keyword in sql_upper for keyword in ddl_keywords)

    if has_ddl:
        # æ£€æŸ¥æ˜¯å¦è·å¾—æˆæƒ
        if not self._has_ddl_permission():
            return {
                "safe": False,
                "error": "DDLæ“ä½œéœ€è¦æ˜¾å¼æˆæƒ",
                "suggestion": "è¯·åœ¨agent_configä¸­å¯ç”¨allow_ddl=true"
            }

    # 2. æ£€æµ‹DMLæ“ä½œ
    dml_keywords = ["UPDATE", "DELETE"]
    has_dml = any(sql_upper.startswith(keyword) for keyword in dml_keywords)

    if has_dml:
        # æ£€æŸ¥æ˜¯å¦æœ‰WHEREæ¡ä»¶
        if "WHERE" not in sql_upper:
            return {
                "safe": False,
                "error": "DMLæ“ä½œç¼ºå°‘WHEREæ¡ä»¶",
                "suggestion": "è¯·æ·»åŠ WHEREæ¡ä»¶é™åˆ¶å½±å“èŒƒå›´"
            }

    # 3. æ£€æµ‹å±é™©æ“ä½œ
    dangerous_keywords = ["DROP DATABASE", "DROP SCHEMA", "TRUNCATE TABLE"]
    has_dangerous = any(keyword in sql_upper for keyword in dangerous_keywords)

    if has_dangerous:
        return {
            "safe": False,
            "error": "æ£€æµ‹åˆ°å±é™©æ“ä½œ",
            "suggestion": "æ­¤æ“ä½œè¢«ç¦æ­¢ï¼Œè¯·æ£€æŸ¥SQL"
        }

    return {"safe": True}
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šåœºæ™¯èƒ½åŠ›å·®è·åˆ†æ

### æ·±åº¦åˆ†æèƒ½åŠ›å·®è·

**ç¼ºå¤±èƒ½åŠ›æ¸…å•**:

| èƒ½åŠ› | æè¿° | ä¼˜å…ˆçº§ |
|------|------|--------|
| æ¢ç´¢å¼åˆ†æ | è‡ªåŠ¨å¤šç»´åº¦åˆ†æã€è¶‹åŠ¿è¯†åˆ«ã€å¼‚å¸¸æ£€æµ‹ | P0 |
| ç»Ÿè®¡åˆ†æ | æè¿°æ€§ç»Ÿè®¡ã€å‡è®¾æ£€éªŒã€ç›¸å…³æ€§åˆ†æã€å›å½’åˆ†æ | P0 |
| å‡è®¾éªŒè¯ | è‡ªåŠ¨ç”Ÿæˆå‡è®¾ã€ç»Ÿè®¡éªŒè¯ã€ç»“æœè§£é‡Š | P0 |
| å¯è§†åŒ– | è‡ªåŠ¨æ¨èå›¾è¡¨ç±»å‹ã€ç”ŸæˆPlotly/Matplotlibå›¾è¡¨ | P0 |
| æŠ¥å‘Šç”Ÿæˆ | HTML/MarkdownæŠ¥å‘Šã€å¤šæ¨¡æ¿ã€åŒ…å«æ´å¯Ÿ | P0 |
| ReActæ¨ç† | å¾ªç¯æ¨ç†ã€å·¥å…·è°ƒç”¨ã€è§‚å¯Ÿ-æ¨ç†-è¡ŒåŠ¨ | P1 |

---

### æ•°æ®è´¨æ£€èƒ½åŠ›å·®è·

**ç¼ºå¤±èƒ½åŠ›æ¸…å•**:

| èƒ½åŠ› | æè¿° | ä¼˜å…ˆçº§ |
|------|------|--------|
| è§„èŒƒæ£€æŸ¥ | å‘½åè§„èŒƒã€å»ºè¡¨è§„èŒƒã€å­—æ®µç±»å‹è§„èŒƒã€åˆ†åŒºè§„èŒƒã€æ³¨é‡Šè§„èŒƒ | P0 |
| æ•°æ®ç”»åƒ | ç»Ÿè®¡ä¿¡æ¯ã€åˆ†å¸ƒåˆ†æã€åŸºæ•°åˆ†æã€æ•°æ®é‡‡æ · | P0 |
| è´¨é‡æ£€æŸ¥ | å®Œæ•´æ€§ã€ä¸€è‡´æ€§ã€å‡†ç¡®æ€§ã€æ—¶æ•ˆæ€§ã€å”¯ä¸€æ€§ | P0 |
| ç´¢å¼•åˆ†æ | ç´¢å¼•ä½¿ç”¨æƒ…å†µã€å†—ä½™æ£€æµ‹ã€ç¼ºå¤±å»ºè®®ã€æ•ˆæœè¯„ä¼° | P0 |
| æ•°æ®æ¼‚ç§» | åˆ†å¸ƒæ¼‚ç§»ã€æ¨¡å¼æ¼‚ç§»ã€è¶‹åŠ¿å˜åŒ– | P1 |
| è´¨æ£€æŠ¥å‘Š | æ±‡æ€»æ‰€æœ‰ç»“æœã€è´¨é‡è¯„åˆ†ã€æ”¹è¿›å»ºè®® | P0 |

---

### æ™ºèƒ½é—®æ•°å¢å¼ºç©ºé—´

**å¢å¼ºå»ºè®®**:

| èƒ½åŠ› | æè¿° | ä¼˜å…ˆçº§ |
|------|------|--------|
| æ•°æ®è§£è¯» | åˆ†ææŸ¥è¯¢ç»“æœã€ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿã€è¶‹åŠ¿è§£è¯» | P1 |
| å¯¹è¯è®°å¿† | é•¿æœŸå¯¹è¯å†å²æŒä¹…åŒ–ã€å‘é‡æ£€ç´¢ã€è¯­ä¹‰æœç´¢ | P1 |
| å¤šè½®ä¼˜åŒ– | ä¸Šä¸‹æ–‡å‹ç¼©ã€æ‘˜è¦ç®¡ç†ã€å¼•ç”¨æ¶ˆè§£ | P2 |

---

### SQLç”Ÿæˆä¼˜åŒ–ç©ºé—´

**ä¼˜åŒ–å»ºè®®**:

| èƒ½åŠ› | æè¿° | ä¼˜å…ˆçº§ |
|------|------|--------|
| æ€§èƒ½ä¼˜åŒ– | SQLæ€§èƒ½åˆ†æã€æ‰§è¡Œè®¡åˆ’æ·±åº¦åˆ†æã€ç“¶é¢ˆè¯†åˆ« | P2 |
| SQLé‡å†™ | ç­‰ä»·SQLé‡å†™ã€è¿æ¥ä¼˜åŒ–ã€å­æŸ¥è¯¢ä¼˜åŒ–ã€èšåˆä¼˜åŒ– | P2 |
| æ‰§è¡Œè®¡åˆ’ | æ·±åº¦æ‰§è¡Œè®¡åˆ’åˆ†æã€æˆæœ¬ä¼°ç®—ã€ç´¢å¼•å»ºè®® | P2 |

---

### SQLå®¡æŸ¥å¢å¼ºç©ºé—´

**å¢å¼ºå»ºè®®**:

| èƒ½åŠ› | æè¿° | ä¼˜å…ˆçº§ |
|------|------|--------|
| ä¸šåŠ¡è§„åˆ™éªŒè¯ | ä¸šåŠ¡é€»è¾‘æ£€æŸ¥ã€æ•°æ®ä¸€è‡´æ€§éªŒè¯ã€ä¸šåŠ¡çº¦æŸ | P2 |
| å®‰å…¨å®¡è®¡ | SQLæ³¨å…¥æ£€æµ‹ã€æƒé™æ£€æŸ¥ã€æ•æ„Ÿæ•°æ®è¯†åˆ« | P2 |
| æœ€ä½³å®è·µ | SQLæœ€ä½³å®è·µæ£€æŸ¥ã€å‘½åè§„èŒƒã€ä»£ç é£æ ¼ | P2 |
| å®¡æŸ¥æŠ¥å‘Š | æ±‡æ€»æ‰€æœ‰å®¡æŸ¥ç»“æœã€æ”¹è¿›å»ºè®®ã€ä¼˜å…ˆçº§æ’åº | P2 |

---

## ç¬¬å››éƒ¨åˆ†ï¼šæ”¹è¿›æ–¹æ¡ˆè®¾è®¡

### æ–°èŠ‚ç‚¹è®¾è®¡ (20ä¸ªèŠ‚ç‚¹)

#### æ·±åº¦åˆ†æèŠ‚ç‚¹ (5ä¸ª)

##### 1. ExploratoryAnalysisNode
```python
class ExploratoryAnalysisNode(Node):
    """
    æ¢ç´¢å¼æ•°æ®åˆ†æèŠ‚ç‚¹

    åŠŸèƒ½:
    - å¤šç»´åº¦è‡ªåŠ¨åˆ†æ
    - è¶‹åŠ¿è¯†åˆ«å’Œå¼‚å¸¸æ£€æµ‹
    - ç›¸å…³æ€§åˆ†æ
    - æ•°æ®åˆ†å¸ƒåˆ†æ

    è¾“å…¥:
    - SQLæŸ¥è¯¢ç»“æœæˆ–è¡¨å
    - åˆ†æç»´åº¦é…ç½®

    è¾“å‡º:
    - æ•°æ®æ¦‚å†µ
    - å¤šç»´åº¦åˆ†æç»“æœ
    - è¶‹åŠ¿å’Œå¼‚å¸¸
    - ç›¸å…³æ€§çŸ©é˜µ
    """

    async def run(self):
        # 1. æ•°æ®æ¦‚å†µåˆ†æ
        data_overview = self._analyze_overview(data)

        # 2. å¤šç»´åº¦åˆ‡åˆ†åˆ†æ
        dimensional_analysis = self._analyze_dimensions(data, dimensions)

        # 3. è¶‹åŠ¿å’Œæ¨¡å¼è¯†åˆ«
        trends = self._identify_trends(data)

        # 4. å¼‚å¸¸æ£€æµ‹
        anomalies = self._detect_anomalies(data)

        # 5. ç›¸å…³æ€§çŸ©é˜µ
        correlations = self._compute_correlations(data)
```

##### 2. StatisticalAnalysisNode
```python
class StatisticalAnalysisNode(Node):
    """
    ç»Ÿè®¡åˆ†æèŠ‚ç‚¹

    åŠŸèƒ½:
    - æè¿°æ€§ç»Ÿè®¡ (å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€åˆ†ä½æ•°)
    - å‡è®¾æ£€éªŒ (t-test, chi-square, ANOVA)
    - ç›¸å…³æ€§åˆ†æ (Pearson, Spearman)
    - å›å½’åˆ†æ
    - æ—¶é—´åºåˆ—åˆ†æ

    è¾“å‡º:
    - ç»Ÿè®¡æ‘˜è¦
    - å‡è®¾æ£€éªŒç»“æœ (på€¼, ç»Ÿè®¡é‡)
    - ç›¸å…³æ€§çŸ©é˜µ
    - å›å½’æ¨¡å‹
    """

    async def run(self):
        # 1. æè¿°æ€§ç»Ÿè®¡
        descriptive_stats = self._compute_descriptive_stats(data)

        # 2. åˆ†å¸ƒæ£€éªŒ
        distribution_tests = self._test_distributions(data)

        # 3. ç›¸å…³æ€§åˆ†æ
        correlations = self._compute_correlations(data)

        # 4. å‡è®¾æ£€éªŒ
        hypothesis_tests = self._perform_hypothesis_tests(data)
```

##### 3. HypothesisTestingNode
```python
class HypothesisTestingNode(Node):
    """
    å‡è®¾ç”Ÿæˆå’ŒéªŒè¯èŠ‚ç‚¹

    åŠŸèƒ½:
    - è‡ªåŠ¨ç”Ÿæˆåˆ†æå‡è®¾
    - ç»Ÿè®¡éªŒè¯å‡è®¾
    - ç»“æœè§£é‡Š

    è¾“å‡º:
    - ç”Ÿæˆçš„å‡è®¾åˆ—è¡¨
    - å‡è®¾æ£€éªŒç»“æœ
    - på€¼å’Œç»Ÿè®¡æ˜¾è‘—æ€§è§£é‡Š
    """

    async def run(self):
        # 1. åŸºäºæ•°æ®ç‰¹å¾ç”Ÿæˆå‡è®¾
        hypotheses = await self._generate_hypotheses(data)

        # 2. é€‰æ‹©åˆé€‚çš„ç»Ÿè®¡æ£€éªŒæ–¹æ³•
        test_methods = self._select_test_methods(hypotheses)

        # 3. æ‰§è¡Œå‡è®¾æ£€éªŒ
        test_results = self._perform_tests(data, hypotheses, test_methods)

        # 4. è§£é‡Špå€¼å’Œç»Ÿè®¡æ˜¾è‘—æ€§
        interpretations = self._interpret_results(test_results)
```

##### 4. VisualizationNode
```python
class VisualizationNode(Node):
    """
    æ•°æ®å¯è§†åŒ–èŠ‚ç‚¹

    åŠŸèƒ½:
    - è‡ªåŠ¨æ¨èå›¾è¡¨ç±»å‹
    - ç”Ÿæˆ Plotly/Matplotlib å›¾è¡¨
    - æ”¯æŒäº¤äº’å¼å›¾è¡¨

    è¾“å‡º:
    - å›¾è¡¨é…ç½® (JSON)
    - å›¾è¡¨HTML
    - å›¾è¡¨æè¿°
    """

    async def run(self):
        # 1. åˆ†ææ•°æ®ç‰¹å¾
        data_features = self._analyze_features(data)

        # 2. æ¨èåˆé€‚çš„å›¾è¡¨ç±»å‹
        chart_types = self._recommend_chart_types(data_features)

        # 3. ç”Ÿæˆå¯è§†åŒ–ä»£ç 
        chart_configs = self._generate_charts(data, chart_types)

        # 4. æ¸²æŸ“å›¾è¡¨ (JSON/HTML)
        charts = self._render_charts(chart_configs)
```

##### 5. ReportGenerationNode
```python
class ReportGenerationNode(Node):
    """
    åˆ†ææŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹

    åŠŸèƒ½:
    - ç”Ÿæˆ HTML/Markdown æŠ¥å‘Š
    - å¤šç§æŠ¥å‘Šæ¨¡æ¿
    - åŒ…å«å›¾è¡¨å’Œæ´å¯Ÿ

    è¾“å‡º:
    - æŠ¥å‘ŠURL
    - æŠ¥å‘Šæ‘˜è¦
    """

    async def run(self):
        # 1. æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_results = self._collect_results()

        # 2. é€‰æ‹©åˆé€‚çš„æŠ¥å‘Šæ¨¡æ¿
        template = self._select_template(analysis_results)

        # 3. ç”Ÿæˆæ´å¯Ÿå’Œç»“è®º
        insights = await self._generate_insights(analysis_results)

        # 4. æ¸²æŸ“æœ€ç»ˆæŠ¥å‘Š
        report = self._render_report(template, analysis_results, insights)
```

---

#### æ•°æ®è´¨æ£€èŠ‚ç‚¹ (6ä¸ª)

##### 1. SchemaStandardsCheckNode
```python
class SchemaStandardsCheckNode(Node):
    """
    æ¨¡å¼è§„èŒƒæ£€æŸ¥èŠ‚ç‚¹

    æ£€æŸ¥é¡¹:
    - å‘½åè§„èŒƒ (è¡¨åã€å­—æ®µå)
    - å»ºè¡¨è§„èŒƒ (ä¸»é”®ã€å¤–é”®ã€ç´¢å¼•)
    - å­—æ®µç±»å‹è§„èŒƒ
    - åˆ†åŒºè§„èŒƒ
    - æ³¨é‡Šè§„èŒƒ

    è¾“å‡º:
    - è§„èŒƒæ£€æŸ¥æŠ¥å‘Š
    - è¿è§„é¡¹åˆ—è¡¨
    - æ”¹è¿›å»ºè®®
    """

    async def run(self):
        # 1. è·å–è¡¨DDL
        ddl = self._get_table_ddl(table_name)

        # 2. åº”ç”¨å‘½åè§„èŒƒè§„åˆ™
        naming_violations = self._check_naming_standards(ddl)

        # 3. åº”ç”¨å»ºè¡¨è§„èŒƒè§„åˆ™
        structure_violations = self._check_structure_standards(ddl)

        # 4. åº”ç”¨å­—æ®µç±»å‹è§„èŒƒ
        type_violations = self._check_type_standards(ddl)

        # 5. ç”Ÿè§„èŒƒæ£€æŸ¥æŠ¥å‘Š
        report = self._generate_report(naming_violations, structure_violations, type_violations)
```

##### 2. DataProfilingNode
```python
class DataProfilingNode(Node):
    """
    æ•°æ®ç”»åƒèŠ‚ç‚¹

    åŠŸèƒ½:
    - ç»Ÿè®¡ä¿¡æ¯ (è¡Œæ•°ã€åˆ—æ•°ã€æ•°æ®ç±»å‹)
    - åˆ†å¸ƒåˆ†æ (ç›´æ–¹å›¾ã€åˆ†ä½æ•°)
    - åŸºæ•°åˆ†æ (å”¯ä¸€å€¼ã€NULLå€¼)
    - æ•°æ®é‡‡æ ·

    è¾“å‡º:
    - æ•°æ®ç”»åƒæŠ¥å‘Š
    - ç»Ÿè®¡æ‘˜è¦
    - é‡‡æ ·æ•°æ®
    """

    async def run(self):
        # 1. åˆ†æè¡¨ç»“æ„
        table_info = self._analyze_table_structure(table_name)

        # 2. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = self._compute_statistics(table_name)

        # 3. é‡‡æ ·æ•°æ®
        samples = self._sample_data(table_name, sample_size=1000)

        # 4. ç”Ÿæˆæ•°æ®ç”»åƒ
        profile = self._generate_profile(table_info, statistics, samples)
```

##### 3. DataQualityCheckNode
```python
class DataQualityCheckNode(Node):
    """
    æ•°æ®è´¨é‡æ£€æŸ¥èŠ‚ç‚¹

    æ£€æŸ¥ç»´åº¦:
    - å®Œæ•´æ€§ (NULLå€¼ã€ç¼ºå¤±å€¼)
    - ä¸€è‡´æ€§ (å¤–é”®çº¦æŸã€æ•°æ®ç±»å‹)
    - å‡†ç¡®æ€§ (æ ¼å¼éªŒè¯ã€èŒƒå›´éªŒè¯)
    - æ—¶æ•ˆæ€§ (æ•°æ®æ–°é²œåº¦)
    - å”¯ä¸€æ€§ (é‡å¤æ•°æ®)

    è¾“å‡º:
    - è´¨é‡æ£€æŸ¥æŠ¥å‘Š
    - è´¨é‡è¯„åˆ† (0-100)
    - é—®é¢˜åˆ—è¡¨
    """

    async def run(self):
        # 1. å®Œæ•´æ€§æ£€æŸ¥
        completeness = self._check_completeness(table_name)

        # 2. ä¸€è‡´æ€§æ£€æŸ¥
        consistency = self._check_consistency(table_name)

        # 3. å‡†ç¡®æ€§æ£€æŸ¥
        accuracy = self._check_accuracy(table_name)

        # 4. æ—¶æ•ˆæ€§æ£€æŸ¥
        timeliness = self._check_timeliness(table_name)

        # 5. å”¯ä¸€æ€§æ£€æŸ¥
        uniqueness = self._check_uniqueness(table_name)

        # 6. ç”Ÿæˆè´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(
            completeness, consistency, accuracy, timeliness, uniqueness
        )
```

##### 4. IndexAnalysisNode
```python
class IndexAnalysisNode(Node):
    """
    ç´¢å¼•åˆ†æèŠ‚ç‚¹

    åŠŸèƒ½:
    - ç´¢å¼•ä½¿ç”¨æƒ…å†µåˆ†æ
    - å†—ä½™ç´¢å¼•æ£€æµ‹
    - ç¼ºå¤±ç´¢å¼•å»ºè®®
    - ç´¢å¼•æ•ˆæœè¯„ä¼°

    è¾“å‡º:
    - ç´¢å¼•åˆ†ææŠ¥å‘Š
    - ä¼˜åŒ–å»ºè®®
    """

    async def run(self):
        # 1. è·å–è¡¨ç´¢å¼•ä¿¡æ¯
        indexes = self._get_indexes(table_name)

        # 2. åˆ†ææŸ¥è¯¢æ¨¡å¼
        query_patterns = self._analyze_query_patterns(table_name)

        # 3. è¯„ä¼°ç´¢å¼•æ•ˆç‡
        effectiveness = self._evaluate_effectiveness(indexes, query_patterns)

        # 4. ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®
        recommendations = self._generate_recommendations(indexes, effectiveness)
```

##### 5. DataDriftDetectionNode
```python
class DataDriftDetectionNode(Node):
    """
    æ•°æ®æ¼‚ç§»æ£€æµ‹èŠ‚ç‚¹

    åŠŸèƒ½:
    - åˆ†å¸ƒæ¼‚ç§»æ£€æµ‹
    - æ¨¡å¼æ¼‚ç§»æ£€æµ‹
    - è¶‹åŠ¿å˜åŒ–æ£€æµ‹

    è¾“å‡º:
    - æ¼‚ç§»æ£€æµ‹æŠ¥å‘Š
    - æ¼‚ç§»æŒ‡æ ‡ (KLæ•£åº¦ã€PSI)
    """

    async def run(self):
        # 1. è·å–å†å²æ•°æ®åˆ†å¸ƒ
        historical_dist = self._get_historical_distribution(table_name)

        # 2. è·å–å½“å‰æ•°æ®åˆ†å¸ƒ
        current_dist = self._get_current_distribution(table_name)

        # 3. è®¡ç®—æ¼‚ç§»æŒ‡æ ‡ (KLæ•£åº¦ã€PSI)
        drift_metrics = self._calculate_drift_metrics(historical_dist, current_dist)

        # 4. ç”Ÿæˆæ¼‚ç§»æŠ¥å‘Š
        report = self._generate_drift_report(drift_metrics)
```

##### 6. QualityReportNode
```python
class QualityReportNode(Node):
    """
    è´¨æ£€æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹

    åŠŸèƒ½:
    - æ±‡æ€»æ‰€æœ‰è´¨æ£€ç»“æœ
    - ç”Ÿæˆ HTML/Markdown æŠ¥å‘Š
    - æä¾›æ”¹è¿›å»ºè®®

    è¾“å‡º:
    - è´¨æ£€æŠ¥å‘ŠURL
    - ç»¼åˆè´¨é‡è¯„åˆ†
    """

    async def run(self):
        # 1. æ”¶é›†æ‰€æœ‰è´¨æ£€èŠ‚ç‚¹ç»“æœ
        quality_results = self._collect_quality_results()

        # 2. è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ†
        overall_score = self._calculate_overall_score(quality_results)

        # 3. ç”Ÿæˆæ”¹è¿›å»ºè®®
        recommendations = self._generate_recommendations(quality_results)

        # 4. æ¸²æŸ“æœ€ç»ˆæŠ¥å‘Š
        report = self._render_report(quality_results, overall_score, recommendations)
```

---

#### æ™ºèƒ½é—®æ•°èŠ‚ç‚¹ (2ä¸ª)

##### 1. DataInterpretationNode
```python
class DataInterpretationNode(Node):
    """
    æ•°æ®è§£è¯»èŠ‚ç‚¹

    åŠŸèƒ½:
    - åˆ†ææŸ¥è¯¢ç»“æœ
    - ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ
    - è¶‹åŠ¿è§£è¯»

    è¾“å‡º:
    - æ•°æ®æ´å¯Ÿ
    - è¶‹åŠ¿è§£è¯»
    - ä¸šåŠ¡å»ºè®®
    """

    async def run(self):
        # 1. åˆ†ææŸ¥è¯¢ç»“æœ
        result_analysis = self._analyze_result(data)

        # 2. ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ
        insights = await self._generate_insights(result_analysis, business_knowledge)

        # 3. è¶‹åŠ¿è§£è¯»
        trend_interpretation = self._interpret_trends(result_analysis)

        # 4. ä¸šåŠ¡å»ºè®®
        recommendations = self._generate_recommendations(insights, trend_interpretation)
```

##### 2. ConversationMemoryNode
```python
class ConversationMemoryNode(Node):
    """
    å¯¹è¯è®°å¿†èŠ‚ç‚¹

    åŠŸèƒ½:
    - ç®¡ç†é•¿æœŸå¯¹è¯å†å²
    - å‘é‡å­˜å‚¨
    - è¯­ä¹‰æ£€ç´¢

    è¾“å‡º:
    - ç›¸å…³å†å²å¯¹è¯
    - ä¸Šä¸‹æ–‡æ‘˜è¦
    """

    async def run(self):
        # 1. å­˜å‚¨å½“å‰å¯¹è¯
        self._store_conversation(current_conversation)

        # 2. è¯­ä¹‰æ£€ç´¢ç›¸å…³å†å²
        relevant_history = self._retrieve_relevant_history(query)

        # 3. ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦
        context_summary = await self._generate_summary(relevant_history)

        # 4. è¿”å›ç›¸å…³å¯¹è¯å’Œæ‘˜è¦
        return {
            "relevant_history": relevant_history,
            "context_summary": context_summary
        }
```

---

#### SQLä¼˜åŒ–èŠ‚ç‚¹ (3ä¸ª)

##### 1. PerformanceOptimizationNode
```python
class PerformanceOptimizationNode(Node):
    """
    SQLæ€§èƒ½ä¼˜åŒ–èŠ‚ç‚¹

    åŠŸèƒ½:
    - SQLæ€§èƒ½åˆ†æ
    - ç“¶é¢ˆè¯†åˆ«
    - ä¼˜åŒ–å»ºè®®

    è¾“å‡º:
    - æ€§èƒ½åˆ†ææŠ¥å‘Š
    - ä¼˜åŒ–å»ºè®®
    """

    async def run(self):
        # 1. æ‰§è¡Œè®¡åˆ’åˆ†æ
        execution_plan = await self._analyze_execution_plan(sql)

        # 2. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
        bottlenecks = self._identify_bottlenecks(execution_plan)

        # 3. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self._generate_optimization_recommendations(bottlenecks)
```

##### 2. SQLRewriteNode
```python
class SQLRewriteNode(Node):
    """
    SQLé‡å†™èŠ‚ç‚¹

    åŠŸèƒ½:
    - ç­‰ä»·SQLé‡å†™
    - è¿æ¥ä¼˜åŒ–
    - å­æŸ¥è¯¢ä¼˜åŒ–
    - èšåˆä¼˜åŒ–

    è¾“å‡º:
    - é‡å†™åçš„SQL
    - ä¼˜åŒ–è¯´æ˜
    """

    async def run(self):
        # 1. åˆ†æSQLç»“æ„
        sql_structure = self._analyze_structure(sql)

        # 2. åº”ç”¨é‡å†™è§„åˆ™
        rewritten_sql = self._apply_rewrite_rules(sql, sql_structure)

        # 3. éªŒè¯ç­‰ä»·æ€§
        equivalence = self._verify_equivalence(sql, rewritten_sql)

        # 4. ç”Ÿæˆä¼˜åŒ–è¯´æ˜
        optimization_notes = self._generate_optimization_notes(sql, rewritten_sql)
```

##### 3. ExecutionPlanAnalysisNode
```python
class ExecutionPlanAnalysisNode(Node):
    """
    æ‰§è¡Œè®¡åˆ’åˆ†æèŠ‚ç‚¹

    åŠŸèƒ½:
    - æ·±åº¦æ‰§è¡Œè®¡åˆ’åˆ†æ
    - æˆæœ¬ä¼°ç®—
    - ç´¢å¼•å»ºè®®

    è¾“å‡º:
    - æ‰§è¡Œè®¡åˆ’åˆ†ææŠ¥å‘Š
    - ç´¢å¼•å»ºè®®
    """

    async def run(self):
        # 1. è§£ææ‰§è¡Œè®¡åˆ’
        execution_plan = await self._parse_execution_plan(sql)

        # 2. æˆæœ¬ä¼°ç®—
        cost_estimation = self._estimate_cost(execution_plan)

        # 3. ç´¢å¼•å»ºè®®
        index_recommendations = self._generate_index_recommendations(execution_plan)

        # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = self._generate_report(execution_plan, cost_estimation, index_recommendations)
```

---

#### SQLå®¡æŸ¥èŠ‚ç‚¹ (4ä¸ª)

##### 1. BusinessRuleValidationNode
```python
class BusinessRuleValidationNode(Node):
    """
    ä¸šåŠ¡è§„åˆ™éªŒè¯èŠ‚ç‚¹

    åŠŸèƒ½:
    - ä¸šåŠ¡é€»è¾‘æ£€æŸ¥
    - æ•°æ®ä¸€è‡´æ€§éªŒè¯
    - ä¸šåŠ¡çº¦æŸéªŒè¯

    è¾“å‡º:
    - ä¸šåŠ¡è§„åˆ™éªŒè¯æŠ¥å‘Š
    - è¿è§„é¡¹åˆ—è¡¨
    """

    async def run(self):
        # 1. åŠ è½½ä¸šåŠ¡è§„åˆ™
        business_rules = self._load_business_rules()

        # 2. éªŒè¯ä¸šåŠ¡é€»è¾‘
        logic_violations = self._validate_business_logic(sql, business_rules)

        # 3. éªŒè¯æ•°æ®ä¸€è‡´æ€§
        consistency_violations = self._validate_data_consistency(sql, business_rules)

        # 4. ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = self._generate_validation_report(logic_violations, consistency_violations)
```

##### 2. SecurityAuditNode
```python
class SecurityAuditNode(Node):
    """
    SQLå®‰å…¨å®¡è®¡èŠ‚ç‚¹

    åŠŸèƒ½:
    - SQLæ³¨å…¥æ£€æµ‹
    - æƒé™æ£€æŸ¥
    - æ•æ„Ÿæ•°æ®è¯†åˆ«

    è¾“å‡º:
    - å®‰å…¨å®¡è®¡æŠ¥å‘Š
    - é£é™©ç­‰çº§
    """

    async def run(self):
        # 1. SQLæ³¨å…¥æ£€æµ‹
        injection_risks = self._detect_sql_injection(sql)

        # 2. æƒé™æ£€æŸ¥
        permission_issues = self._check_permissions(sql)

        # 3. æ•æ„Ÿæ•°æ®è¯†åˆ«
        sensitive_data = self._identify_sensitive_data(sql)

        # 4. ç”Ÿæˆå®‰å…¨å®¡è®¡æŠ¥å‘Š
        report = self._generate_security_report(injection_risks, permission_issues, sensitive_data)
```

##### 3. BestPracticeCheckNode
```python
class BestPracticeCheckNode(Node):
    """
    æœ€ä½³å®è·µæ£€æŸ¥èŠ‚ç‚¹

    åŠŸèƒ½:
    - SQLæœ€ä½³å®è·µæ£€æŸ¥
    - å‘½åè§„èŒƒæ£€æŸ¥
    - ä»£ç é£æ ¼æ£€æŸ¥

    è¾“å‡º:
    - æœ€ä½³å®è·µæ£€æŸ¥æŠ¥å‘Š
    - æ”¹è¿›å»ºè®®
    """

    async def run(self):
        # 1. åŠ è½½æœ€ä½³å®è·µè§„åˆ™
        best_practices = self._load_best_practices()

        # 2. æ£€æŸ¥SQLæœ€ä½³å®è·µ
        practice_violations = self._check_best_practices(sql, best_practices)

        # 3. æ£€æŸ¥å‘½åè§„èŒƒ
        naming_violations = self._check_naming_conventions(sql)

        # 4. æ£€æŸ¥ä»£ç é£æ ¼
        style_violations = self._check_code_style(sql)

        # 5. ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š
        report = self._generate_check_report(practice_violations, naming_violations, style_violations)
```

##### 4. ReviewReportNode
```python
class ReviewReportNode(Node):
    """
    å®¡æŸ¥æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹

    åŠŸèƒ½:
    - æ±‡æ€»æ‰€æœ‰å®¡æŸ¥ç»“æœ
    - ç”Ÿæˆ HTML/Markdown æŠ¥å‘Š
    - ä¼˜å…ˆçº§æ’åº

    è¾“å‡º:
    - å®¡æŸ¥æŠ¥å‘ŠURL
    - é—®é¢˜æ±‡æ€»
    """

    async def run(self):
        # 1. æ”¶é›†æ‰€æœ‰å®¡æŸ¥ç»“æœ
        review_results = self._collect_review_results()

        # 2. ä¼˜å…ˆçº§æ’åº
        prioritized_issues = self._prioritize_issues(review_results)

        # 3. ç”ŸæˆæŠ¥å‘Š
        report = self._generate_review_report(review_results, prioritized_issues)

        # 4. è¿”å›æŠ¥å‘ŠURL
        return {"report_url": report.url, "issues": prioritized_issues}
```

---

### æ–°å·¥ä½œæµè®¾è®¡

#### æ·±åº¦åˆ†æå·¥ä½œæµ
```yaml
# workflow.yml æ–°å¢
deep_analysis:
  - intent_analysis         # ä»»åŠ¡ç±»å‹ç¡®è®¤
  - intent_clarification    # ä¸šåŠ¡æ„å›¾æ¾„æ¸…
  - exploratory_analysis    # æ¢ç´¢å¼åˆ†æ
  - statistical_analysis    # ç»Ÿè®¡åˆ†æ
  - hypothesis_testing      # å‡è®¾éªŒè¯
  - visualization           # å¯è§†åŒ–ç”Ÿæˆ
  - report_generation       # æŠ¥å‘Šç”Ÿæˆ
  - output                  # è¾“å‡ºç»“æœ
```

#### æ•°æ®è´¨æ£€å·¥ä½œæµ
```yaml
# workflow.yml æ–°å¢
data_quality:
  - schema_standards_check  # è§„èŒƒæ£€æŸ¥
  - data_profiling          # æ•°æ®ç”»åƒ
  - data_quality_check      # è´¨é‡æ£€æŸ¥
  - index_analysis          # ç´¢å¼•åˆ†æ
  - quality_report          # è´¨æ£€æŠ¥å‘Š
  - output                  # è¾“å‡ºç»“æœ
```

#### æ™ºèƒ½é—®æ•°å¢å¼ºå·¥ä½œæµ
```yaml
# workflow.yml ä¿®æ”¹
text2sql_enhanced:
  - intent_analysis
  - intent_clarification
  - conversation_memory     # æ–°å¢ï¼šå¯¹è¯è®°å¿†
  - schema_discovery
  - schema_validation
  - generate_sql
  - sql_validate
  - execute_sql
  - data_interpretation     # æ–°å¢ï¼šæ•°æ®è§£è¯»
  - result_validation
  - reflect
  - output
```

---

### API å¢å¼º

#### RunWorkflowRequest æ‰©å±•
```python
# datus/schemas/api_models.py

class RunWorkflowRequest(BaseModel):
    # ç°æœ‰å­—æ®µ...
    workflow: str
    namespace: str
    task: str
    database_name: Optional[str] = None
    domain: Optional[str] = None
    layer1: Optional[str] = None
    layer2: Optional[str] = None
    ext_knowledge: Optional[str] = None
    current_date: Optional[str] = None
    plan_mode: Optional[bool] = False

    # æ–°å¢å­—æ®µ
    output_format: Optional[str] = Field(
        "json",
        description="Output format: json/markdown/html"
    )
    analysis_depth: Optional[str] = Field(
        "standard",
        description="Analysis depth: basic/standard/deep"
    )
    include_visualization: Optional[bool] = Field(
        False,
        description="Include data visualization in output"
    )
    include_insights: Optional[bool] = Field(
        False,
        description="Include AI-generated insights in output"
    )
    max_execution_time: Optional[int] = Field(
        300,
        description="Maximum execution time in seconds"
    )
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šå®æ–½è·¯çº¿å›¾

### Phase 1: æ·±åº¦åˆ†æèƒ½åŠ›å»ºè®¾ (4-6å‘¨)

**ä¼˜å…ˆçº§: P0 (æœ€é«˜)**

**ç›®æ ‡**: æ„å»ºå®Œæ•´çš„æ·±åº¦åˆ†æèƒ½åŠ›ï¼Œæ”¯æŒæ¢ç´¢å¼æ•°æ®åˆ†æ

**ä»»åŠ¡åˆ—è¡¨**:

#### Week 1-2: æ ¸å¿ƒåˆ†æèŠ‚ç‚¹
- [ ] ExploratoryAnalysisNode å®ç°
  - æ•°æ®æ¦‚å†µåˆ†æ
  - å¤šç»´åº¦åˆ‡åˆ†åˆ†æ
  - è¶‹åŠ¿è¯†åˆ«å’Œå¼‚å¸¸æ£€æµ‹
  - ç›¸å…³æ€§çŸ©é˜µè®¡ç®—
- [ ] StatisticalAnalysisNode å®ç°
  - æè¿°æ€§ç»Ÿè®¡è®¡ç®—
  - åˆ†å¸ƒæ£€éªŒ (æ­£æ€æ€§ã€ååº¦ã€å³°åº¦)
  - ç›¸å…³æ€§åˆ†æ (Pearson, Spearman)
  - å‡è®¾æ£€éªŒ (t-test, chi-square, ANOVA)
- [ ] HypothesisTestingNode å®ç°
  - è‡ªåŠ¨å‡è®¾ç”Ÿæˆ (åŸºäºæ•°æ®ç‰¹å¾)
  - ç»Ÿè®¡æ£€éªŒæ–¹æ³•é€‰æ‹©
  - på€¼è®¡ç®—å’Œè§£é‡Š
  - ç»Ÿè®¡æ˜¾è‘—æ€§åˆ¤æ–­

#### Week 3-4: å¯è§†åŒ–å’ŒæŠ¥å‘Š
- [ ] VisualizationNode å®ç°
  - æ•°æ®ç‰¹å¾åˆ†æ
  - å›¾è¡¨ç±»å‹æ¨èç®—æ³•
  - Plotly å›¾è¡¨ç”Ÿæˆ
  - äº¤äº’å¼å›¾è¡¨æ”¯æŒ
- [ ] ReportGenerationNode å®ç°
  - æŠ¥å‘Šæ¨¡æ¿åº“å»ºè®¾
  - æ´å¯Ÿç”Ÿæˆç®—æ³•
  - HTML/Markdown æŠ¥å‘Šæ¸²æŸ“
  - æŠ¥å‘Šæ ·å¼å®šåˆ¶

#### Week 5-6: å·¥ä½œæµé›†æˆ
- [ ] deep_analysis å·¥ä½œæµé…ç½®
- [ ] Prompt æ¨¡æ¿å¼€å‘
  - `deep_analysis_system_1.0.j2`
  - `statistical_analysis_system_1.0.j2`
  - `exploratory_analysis_system_1.0.j2`
- [ ] æµ‹è¯•å’Œä¼˜åŒ–
  - å•å…ƒæµ‹è¯• (æ¯ä¸ªèŠ‚ç‚¹)
  - é›†æˆæµ‹è¯• (ç«¯åˆ°ç«¯å·¥ä½œæµ)
  - æ€§èƒ½ä¼˜åŒ–

**å…³é”®æ–‡ä»¶**:
- `datus/agent/node/exploratory_analysis_node.py`
- `datus/agent/node/statistical_analysis_node.py`
- `datus/agent/node/hypothesis_testing_node.py`
- `datus/agent/node/visualization_node.py`
- `datus/agent/node/report_generation_node.py`
- `datus/prompts/deep_analysis_system_1.0.j2`
- `datus/prompts/statistical_analysis_system_1.0.j2`

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ”¯æŒç«¯åˆ°ç«¯çš„æ·±åº¦åˆ†æå·¥ä½œæµ
- [ ] è‡ªåŠ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š (HTMLæ ¼å¼)
- [ ] åŒ…å«å¯è§†åŒ–å›¾è¡¨ (è‡³å°‘3ç§ç±»å‹)
- [ ] é€šè¿‡10+æµ‹è¯•ç”¨ä¾‹
- [ ] ç»Ÿè®¡æ£€éªŒå‡†ç¡®ç‡ > 90%

---

### Phase 2: æ•°æ®è´¨æ£€èƒ½åŠ›å»ºè®¾ (3-4å‘¨)

**ä¼˜å…ˆçº§: P0 (æœ€é«˜)**

**ç›®æ ‡**: æ„å»ºå…¨é¢çš„æ•°æ®è´¨æ£€èƒ½åŠ›ï¼Œæ”¯æŒè§„èŒƒæ£€æŸ¥å’Œè´¨é‡ç›‘æ§

**ä»»åŠ¡åˆ—è¡¨**:

#### Week 1: è§„èŒƒæ£€æŸ¥èŠ‚ç‚¹
- [ ] SchemaStandardsCheckNode å®ç°
  - å‘½åè§„èŒƒæ£€æŸ¥ (è¡¨åã€å­—æ®µå)
  - å»ºè¡¨è§„èŒƒæ£€æŸ¥ (ä¸»é”®ã€å¤–é”®ã€ç´¢å¼•)
  - å­—æ®µç±»å‹è§„èŒƒæ£€æŸ¥
  - åˆ†åŒºè§„èŒƒæ£€æŸ¥
  - æ³¨é‡Šè§„èŒƒæ£€æŸ¥
- [ ] è§„åˆ™åº“å»ºè®¾
  - `datus/rules/schema_standards.yaml`
  - å¯é…ç½®è§„åˆ™å¼•æ“
  - è§„åˆ™ä¼˜å…ˆçº§å’Œä¸¥é‡ç¨‹åº¦
- [ ] é…ç½®åŒ–æ”¯æŒ
  - è§„åˆ™çƒ­æ›´æ–°
  - è‡ªå®šä¹‰è§„åˆ™æ”¯æŒ

#### Week 2: è´¨é‡æ£€æŸ¥èŠ‚ç‚¹
- [ ] DataProfilingNode å®ç°
  - è¡¨ç»“æ„åˆ†æ
  - ç»Ÿè®¡ä¿¡æ¯è®¡ç®—
  - åˆ†å¸ƒåˆ†æ (ç›´æ–¹å›¾ã€åˆ†ä½æ•°)
  - åŸºæ•°åˆ†æ (å”¯ä¸€å€¼ã€NULLå€¼)
  - æ•°æ®é‡‡æ ·
- [ ] DataQualityCheckNode å®ç°
  - å®Œæ•´æ€§æ£€æŸ¥ (NULLå€¼ã€ç¼ºå¤±å€¼)
  - ä¸€è‡´æ€§æ£€æŸ¥ (å¤–é”®çº¦æŸã€æ•°æ®ç±»å‹)
  - å‡†ç¡®æ€§æ£€æŸ¥ (æ ¼å¼éªŒè¯ã€èŒƒå›´éªŒè¯)
  - æ—¶æ•ˆæ€§æ£€æŸ¥ (æ•°æ®æ–°é²œåº¦)
  - å”¯ä¸€æ€§æ£€æŸ¥ (é‡å¤æ•°æ®)
- [ ] è´¨é‡è¯„åˆ†ç®—æ³•
  - å¤šç»´åº¦åŠ æƒè¯„åˆ†
  - é˜ˆå€¼é…ç½®
  - è¶‹åŠ¿è·Ÿè¸ª

#### Week 3: åˆ†æèŠ‚ç‚¹
- [ ] IndexAnalysisNode å®ç°
  - ç´¢å¼•ä½¿ç”¨æƒ…å†µåˆ†æ
  - å†—ä½™ç´¢å¼•æ£€æµ‹
  - ç¼ºå¤±ç´¢å¼•å»ºè®®
  - ç´¢å¼•æ•ˆæœè¯„ä¼°
- [ ] DataDriftDetectionNode å®ç°
  - å†å²æ•°æ®åˆ†å¸ƒè·å–
  - å½“å‰æ•°æ®åˆ†å¸ƒè®¡ç®—
  - KLæ•£åº¦ã€PSIè®¡ç®—
  - æ¼‚ç§»è¶‹åŠ¿åˆ†æ

#### Week 4: æŠ¥å‘Šå’Œé›†æˆ
- [ ] QualityReportNode å®ç°
  - æ”¶é›†æ‰€æœ‰è´¨æ£€ç»“æœ
  - ç»¼åˆè´¨é‡è¯„åˆ†
  - æ”¹è¿›å»ºè®®ç”Ÿæˆ
  - æŠ¥å‘Šæ¸²æŸ“
- [ ] data_quality å·¥ä½œæµé…ç½®
- [ ] æµ‹è¯•å’Œä¼˜åŒ–

**å…³é”®æ–‡ä»¶**:
- `datus/agent/node/schema_standards_check_node.py`
- `datus/agent/node/data_profiling_node.py`
- `datus/agent/node/data_quality_check_node.py`
- `datus/agent/node/index_analysis_node.py`
- `datus/agent/node/data_drift_detection_node.py`
- `datus/agent/node/quality_report_node.py`
- `datus/rules/schema_standards.yaml`
- `datus/rules/data_quality_rules.yaml`

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ”¯æŒç«¯åˆ°ç«¯çš„æ•°æ®è´¨æ£€å·¥ä½œæµ
- [ ] è‡ªåŠ¨ç”Ÿæˆè´¨æ£€æŠ¥å‘Š (HTMLæ ¼å¼)
- [ ] è´¨é‡è¯„åˆ†å‡†ç¡®æ€§ > 90%
- [ ] é€šè¿‡10+æµ‹è¯•ç”¨ä¾‹
- [ ] è§„èŒƒæ£€æŸ¥è¦†ç›–ç‡ > 95%

---

### Phase 3: æ™ºèƒ½é—®æ•°å¢å¼º (2-3å‘¨)

**ä¼˜å…ˆçº§: P1 (é«˜)**

**ç›®æ ‡**: å¢å¼ºæ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼Œæä¾›æ›´å¥½çš„æ•°æ®æ´å¯Ÿ

**ä»»åŠ¡åˆ—è¡¨**:

#### Week 1: æ•°æ®æ´å¯ŸèŠ‚ç‚¹
- [ ] DataInterpretationNode å®ç°
  - æŸ¥è¯¢ç»“æœåˆ†æ
  - ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆ (LLM-based)
  - è¶‹åŠ¿è§£è¯»
  - ä¸šåŠ¡å»ºè®®ç”Ÿæˆ
- [ ] æ´å¯Ÿç”Ÿæˆç®—æ³•
  - ç»“æœæ‘˜è¦ç®—æ³•
  - è¶‹åŠ¿è¯†åˆ«ç®—æ³•
  - å¼‚å¸¸æ£€æµ‹ç®—æ³•
- [ ] ä¸šåŠ¡çŸ¥è¯†åº“é›†æˆ
  - ä¸šåŠ¡æœ¯è¯­æ˜ å°„
  - æŒ‡æ ‡åº“é›†æˆ

#### Week 2: å¯¹è¯è®°å¿†èŠ‚ç‚¹
- [ ] ConversationMemoryNode å®ç°
  - å¯¹è¯å†å²å­˜å‚¨ (SQLite/PostgreSQL)
  - å‘é‡åŒ– (Embedding)
  - è¯­ä¹‰æ£€ç´¢ (å‘é‡ç›¸ä¼¼åº¦)
  - ä¸Šä¸‹æ–‡å‹ç¼©
- [ ] å‘é‡å­˜å‚¨é›†æˆ
  - å‘é‡æ•°æ®åº“é€‰æ‹© (Milvus/Qdrant)
  - Embeddingæ¨¡å‹é€‰æ‹©
  - å‘é‡ç´¢å¼•ä¼˜åŒ–
- [ ] è¯­ä¹‰æ£€ç´¢
  - ç›¸å…³å¯¹è¯æ£€ç´¢
  - ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„åˆ†

#### Week 3: é›†æˆå’Œä¼˜åŒ–
- [ ] text2sql å·¥ä½œæµå¢å¼º
  - é›†æˆ DataInterpretationNode
  - é›†æˆ ConversationMemoryNode
- [ ] å¤šè½®å¯¹è¯ä¼˜åŒ–
  - å¼•ç”¨æ¶ˆè§£
  - ä¸Šä¸‹æ–‡ä¸€è‡´æ€§
  - å¯¹è¯çŠ¶æ€ç®¡ç†
- [ ] æµ‹è¯•å’Œä¼˜åŒ–

**å…³é”®æ–‡ä»¶**:
- `datus/agent/node/data_interpretation_node.py`
- `datus/agent/node/conversation_memory_node.py`
- `datus/storage/conversation_memory/`
- `datus/prompts/data_interpretation_system_1.0.j2`

**éªŒæ”¶æ ‡å‡†**:
- [ ] è‡ªåŠ¨ç”Ÿæˆæ•°æ®æ´å¯Ÿ
- [ ] æ”¯æŒ10+è½®å¯¹è¯è®°å¿†
- [ ] æ´å¯Ÿç›¸å…³æ€§ > 80%
- [ ] å¯¹è¯ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ > 90%

---

### Phase 4: SQLç”Ÿæˆ/å®¡æŸ¥ä¼˜åŒ– (2-3å‘¨)

**ä¼˜å…ˆçº§: P2 (ä¸­)**

**ç›®æ ‡**: ä¼˜åŒ–SQLç”Ÿæˆå’Œå®¡æŸ¥èƒ½åŠ›ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½å’Œå®‰å…¨æ€§

**ä»»åŠ¡åˆ—è¡¨**:

#### Week 1: æ€§èƒ½ä¼˜åŒ–èŠ‚ç‚¹
- [ ] PerformanceOptimizationNode å®ç°
  - æ‰§è¡Œè®¡åˆ’æ·±åº¦åˆ†æ
  - æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
  - ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
- [ ] SQLRewriteNode å®ç°
  - SQLç»“æ„åˆ†æ
  - é‡å†™è§„åˆ™å¼•æ“
  - ç­‰ä»·æ€§éªŒè¯
- [ ] ExecutionPlanAnalysisNode å®ç°
  - æ‰§è¡Œè®¡åˆ’è§£æ
  - æˆæœ¬ä¼°ç®—
  - ç´¢å¼•å»ºè®®

#### Week 2: å®‰å…¨å’Œè§„èŒƒèŠ‚ç‚¹
- [ ] BusinessRuleValidationNode å®ç°
  - ä¸šåŠ¡è§„åˆ™åŠ è½½
  - ä¸šåŠ¡é€»è¾‘éªŒè¯
  - æ•°æ®ä¸€è‡´æ€§éªŒè¯
- [ ] SecurityAuditNode å®ç°
  - SQLæ³¨å…¥æ£€æµ‹
  - æƒé™æ£€æŸ¥
  - æ•æ„Ÿæ•°æ®è¯†åˆ«
- [ ] BestPracticeCheckNode å®ç°
  - æœ€ä½³å®è·µè§„åˆ™åº“
  - å‘½åè§„èŒƒæ£€æŸ¥
  - ä»£ç é£æ ¼æ£€æŸ¥

#### Week 3: é›†æˆå’Œä¼˜åŒ–
- [ ] å·¥ä½œæµå¢å¼º
  - text2sql å·¥ä½œæµé›†æˆæ€§èƒ½ä¼˜åŒ–
  - sql_review å·¥ä½œæµé›†æˆå®‰å…¨å®¡è®¡
- [ ] è§„åˆ™åº“å»ºè®¾
  - `datus/rules/best_practices.yaml`
  - `datus/rules/business_rules.yaml`
  - `datus/rules/security_rules.yaml`
- [ ] æµ‹è¯•å’Œä¼˜åŒ–

**å…³é”®æ–‡ä»¶**:
- `datus/agent/node/performance_optimization_node.py`
- `datus/agent/node/sql_rewrite_node.py`
- `datus/agent/node/execution_plan_analysis_node.py`
- `datus/agent/node/business_rule_validation_node.py`
- `datus/agent/node/security_audit_node.py`
- `datus/agent/node/best_practice_check_node.py`
- `datus/agent/node/review_report_node.py`
- `datus/prompts/performance_optimization_system_1.0.j2`
- `datus/prompts/security_audit_system_1.0.j2`
- `datus/prompts/best_practice_check_system_1.0.j2`

**éªŒæ”¶æ ‡å‡†**:
- [ ] SQLæ€§èƒ½ä¼˜åŒ–å»ºè®®å‡†ç¡®ç‡ > 85%
- [ ] SQLé‡å†™ç­‰ä»·æ€§ > 95%
- [ ] å®‰å…¨å®¡è®¡è¦†ç›–10+é£é™©ç±»å‹
- [ ] æœ€ä½³å®è·µæ£€æŸ¥è¦†ç›–ç‡ > 90%

---

### Phase 5: æ¶æ„ä¼˜åŒ–å’Œç”Ÿæ€å»ºè®¾ (4-6å‘¨)

**ä¼˜å…ˆçº§: P3 (ä½)**

**ç›®æ ‡**: ä¼˜åŒ–æ¶æ„ï¼Œå»ºè®¾ç”Ÿæ€ï¼Œæä¾›æ›´å¥½çš„æ‰©å±•æ€§

**ä»»åŠ¡åˆ—è¡¨**:

#### Week 1-2: æ’ä»¶åŒ–æ¶æ„
- [ ] èŠ‚ç‚¹æ’ä»¶ç³»ç»Ÿå®ç°
  - æ’ä»¶åŠ è½½æœºåˆ¶
  - æ’ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†
  - æ’ä»¶ä¾èµ–ç®¡ç†
- [ ] åŠ¨æ€åŠ è½½æœºåˆ¶
  - çƒ­åŠ è½½æ”¯æŒ
  - ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
  - é”™è¯¯éš”ç¦»
- [ ] èŠ‚ç‚¹æ³¨å†Œè¡¨è‡ªåŠ¨å‘ç°
  - è‡ªåŠ¨æ‰«ææ’ä»¶ç›®å½•
  - èŠ‚ç‚¹ç±»å‹è‡ªåŠ¨æ³¨å†Œ
  - æ’ä»¶å…ƒæ•°æ®ç®¡ç†

#### Week 3-4: é…ç½®ç®€åŒ–
- [ ] å¯è§†åŒ–å·¥ä½œæµç¼–è¾‘å™¨
  - æ‹–æ‹½å¼èŠ‚ç‚¹ç¼–è¾‘
  - è¿çº¿å¯è§†åŒ–
  - å®æ—¶é¢„è§ˆ
- [ ] é…ç½®å‘å¯¼å’Œæ¨¡æ¿
  - åœºæ™¯æ¨¡æ¿åº“
  - é…ç½®å‘å¯¼
  - å¿«é€Ÿå¼€å§‹æŒ‡å—
- [ ] æ™ºèƒ½é»˜è®¤å€¼
  - åŸºäºåœºæ™¯çš„é»˜è®¤é…ç½®
  - è‡ªåŠ¨æ¨èèŠ‚ç‚¹
  - é…ç½®ä¼˜åŒ–å»ºè®®

#### Week 5-6: ç›‘æ§å’Œå‘Šè­¦
- [ ] å®æ—¶æ€§èƒ½ç›‘æ§
  - èŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ç›‘æ§
  - å†…å­˜ä½¿ç”¨ç›‘æ§
  - å¹¶å‘ç›‘æ§
- [ ] èŠ‚ç‚¹æ‰§è¡Œè¿½è¸ª
  - æ‰§è¡Œé“¾è·¯è¿½è¸ª
  - æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
  - é”™è¯¯è¿½è¸ª
- [ ] å¼‚å¸¸å‘Šè­¦ç³»ç»Ÿ
  - å‘Šè­¦è§„åˆ™é…ç½®
  - å¤šæ¸ é“å‘Šè­¦ (é‚®ä»¶/é’‰é’‰/ä¼å¾®)
  - å‘Šè­¦èšåˆå’Œé™å™ª

**å…³é”®æ–‡ä»¶**:
- `datus/agent/node/plugin_system.py`
- `datus/configuration/node_registry.py`
- `datus/api/workflow_editor.py`
- `datus/monitoring/performance_monitor.py`
- `datus/monitoring/execution_tracker.py`
- `datus/monitoring/alert_system.py`

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ”¯æŒç¬¬ä¸‰æ–¹èŠ‚ç‚¹æ’ä»¶
- [ ] æ’ä»¶çƒ­åŠ è½½ < 5ç§’
- [ ] å¯è§†åŒ–ç¼–è¾‘å™¨å“åº”æ—¶é—´ < 100ms
- [ ] ç›‘æ§è¦†ç›–æ‰€æœ‰èŠ‚ç‚¹
- [ ] å‘Šè­¦å“åº”æ—¶é—´ < 1åˆ†é’Ÿ

---

## é™„å½•

### å…³é”®æ–‡ä»¶æ¸…å•

#### éœ€è¦åˆ›å»ºçš„æ–°æ–‡ä»¶ (48ä¸ª)

**æ·±åº¦åˆ†æèŠ‚ç‚¹ (5ä¸ª)**:
- `datus/agent/node/exploratory_analysis_node.py`
- `datus/agent/node/statistical_analysis_node.py`
- `datus/agent/node/hypothesis_testing_node.py`
- `datus/agent/node/visualization_node.py`
- `datus/agent/node/report_generation_node.py`

**æ•°æ®è´¨æ£€èŠ‚ç‚¹ (6ä¸ª)**:
- `datus/agent/node/schema_standards_check_node.py`
- `datus/agent/node/data_profiling_node.py`
- `datus/agent/node/data_quality_check_node.py`
- `datus/agent/node/index_analysis_node.py`
- `datus/agent/node/data_drift_detection_node.py`
- `datus/agent/node/quality_report_node.py`

**æ™ºèƒ½é—®æ•°èŠ‚ç‚¹ (2ä¸ª)**:
- `datus/agent/node/data_interpretation_node.py`
- `datus/agent/node/conversation_memory_node.py`

**SQLä¼˜åŒ–èŠ‚ç‚¹ (3ä¸ª)**:
- `datus/agent/node/performance_optimization_node.py`
- `datus/agent/node/sql_rewrite_node.py`
- `datus/agent/node/execution_plan_analysis_node.py`

**SQLå®¡æŸ¥èŠ‚ç‚¹ (4ä¸ª)**:
- `datus/agent/node/business_rule_validation_node.py`
- `datus/agent/node/security_audit_node.py`
- `datus/agent/node/best_practice_check_node.py`
- `datus/agent/node/review_report_node.py`

**Schema æ¨¡å‹ (20ä¸ª)**:
- `datus/schemas/exploratory_analysis_node_models.py`
- `datus/schemas/statistical_analysis_node_models.py`
- `datus/schemas/hypothesis_testing_node_models.py`
- `datus/schemas/visualization_node_models.py`
- `datus/schemas/report_generation_node_models.py`
- `datus/schemas/schema_standards_check_node_models.py`
- `datus/schemas/data_profiling_node_models.py`
- `datus/schemas/data_quality_check_node_models.py`
- `datus/schemas/index_analysis_node_models.py`
- `datus/schemas/data_drift_detection_node_models.py`
- `datus/schemas/quality_report_node_models.py`
- `datus/schemas/data_interpretation_node_models.py`
- `datus/schemas/conversation_memory_node_models.py`
- `datus/schemas/performance_optimization_node_models.py`
- `datus/schemas/sql_rewrite_node_models.py`
- `datus/schemas/execution_plan_analysis_node_models.py`
- `datus/schemas/business_rule_validation_node_models.py`
- `datus/schemas/security_audit_node_models.py`
- `datus/schemas/best_practice_check_node_models.py`
- `datus/schemas/review_report_node_models.py`

**Prompt æ¨¡æ¿ (8ä¸ª)**:
- `datus/prompts/deep_analysis_system_1.0.j2`
- `datus/prompts/statistical_analysis_system_1.0.j2`
- `datus/prompts/exploratory_analysis_system_1.0.j2`
- `datus/prompts/data_quality_check_system_1.0.j2`
- `datus/prompts/data_interpretation_system_1.0.j2`
- `datus/prompts/performance_optimization_system_1.0.j2`
- `datus/prompts/security_audit_system_1.0.j2`
- `datus/prompts/best_practice_check_system_1.0.j2`

**é…ç½®æ–‡ä»¶ (5ä¸ª)**:
- `datus/rules/schema_standards.yaml`
- `datus/rules/data_quality_rules.yaml`
- `datus/rules/best_practices.yaml`
- `datus/rules/business_rules.yaml`
- `datus/rules/security_rules.yaml`

**å­˜å‚¨æ¨¡å— (1ä¸ª)**:
- `datus/storage/conversation_memory/__init__.py`

#### éœ€è¦ä¿®æ”¹çš„ç°æœ‰æ–‡ä»¶ (5ä¸ª)

- `datus/configuration/node_type.py` - æ–°å¢20ä¸ªèŠ‚ç‚¹ç±»å‹
- `datus/agent/node/__init__.py` - å¯¼å‡º20ä¸ªæ–°èŠ‚ç‚¹
- `datus/agent/node/node.py` - æ·»åŠ 20ä¸ªæ–°èŠ‚ç‚¹å·¥å‚æ–¹æ³•
- `datus/agent/workflow.yml` - æ–°å¢2ä¸ªå·¥ä½œæµé…ç½®
- `datus/schemas/api_models.py` - APIæ¨¡å‹æ‰©å±• (5ä¸ªæ–°å­—æ®µ)

---

### æµ‹è¯•åœºæ™¯

#### åœºæ™¯1: æ·±åº¦åˆ†æ E2E æµ‹è¯•

```python
def test_deep_analysis_workflow():
    """
    æµ‹è¯•å®Œæ•´çš„æ·±åº¦åˆ†æå·¥ä½œæµ
    """
    request = RunWorkflowRequest(
        workflow="deep_analysis",
        namespace="test_namespace",
        task="åˆ†æé”€å”®æ•°æ®çš„è¶‹åŠ¿ã€å¼‚å¸¸å’Œç›¸å…³æ€§ï¼Œç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š",
        database_name="sales_db",
        analysis_depth="deep",
        include_visualization=True,
        include_insights=True
    )

    response = client.post("/workflows/run", json=request.dict())

    assert response.status_code == 200
    result = response.json()

    # éªŒè¯å·¥ä½œæµå®Œæˆ
    assert result["status"] == "success"

    # éªŒè¯åŒ…å«æ¢ç´¢å¼åˆ†æç»“æœ
    assert "exploratory_analysis" in result["data"]
    assert result["data"]["exploratory_analysis"]["trends"] is not None

    # éªŒè¯åŒ…å«ç»Ÿè®¡åˆ†æç»“æœ
    assert "statistical_analysis" in result["data"]
    assert "correlations" in result["data"]["statistical_analysis"]

    # éªŒè¯åŒ…å«å¯è§†åŒ–
    assert "visualizations" in result["data"]
    assert len(result["data"]["visualizations"]) > 0

    # éªŒè¯åŒ…å«æŠ¥å‘Š
    assert "report_url" in result["data"]
```

#### åœºæ™¯2: æ•°æ®è´¨æ£€ E2E æµ‹è¯•

```python
def test_data_quality_workflow():
    """
    æµ‹è¯•å®Œæ•´çš„æ•°æ®è´¨æ£€å·¥ä½œæµ
    """
    request = RunWorkflowRequest(
        workflow="data_quality",
        namespace="test_namespace",
        task="æ£€æŸ¥ç”¨æˆ·è¡¨çš„å»ºè¡¨è§„èŒƒå’Œæ•°æ®è´¨é‡",
        database_name="user_db",
        table_name="users"
    )

    response = client.post("/workflows/run", json=request.dict())

    assert response.status_code == 200
    result = response.json()

    # éªŒè¯å·¥ä½œæµå®Œæˆ
    assert result["status"] == "success"

    # éªŒè¯åŒ…å«è§„èŒƒæ£€æŸ¥ç»“æœ
    assert "schema_standards_check" in result["data"]
    assert "violations" in result["data"]["schema_standards_check"]

    # éªŒè¯åŒ…å«æ•°æ®ç”»åƒ
    assert "data_profiling" in result["data"]
    assert "statistics" in result["data"]["data_profiling"]

    # éªŒè¯åŒ…å«è´¨é‡æ£€æŸ¥ç»“æœ
    assert "data_quality_check" in result["data"]
    assert "quality_score" in result["data"]["data_quality_check"]
    assert 0 <= result["data"]["data_quality_check"]["quality_score"] <= 100

    # éªŒè¯åŒ…å«ç´¢å¼•åˆ†æ
    assert "index_analysis" in result["data"]

    # éªŒè¯åŒ…å«è´¨æ£€æŠ¥å‘Š
    assert "quality_report_url" in result["data"]
```

#### åœºæ™¯3: æ™ºèƒ½é—®æ•° E2E æµ‹è¯•

```python
def test_smart_qa_with_memory_workflow():
    """
    æµ‹è¯•æ™ºèƒ½é—®æ•°å¢å¼ºå·¥ä½œæµ (å¸¦å¯¹è¯è®°å¿†)
    """
    # ç¬¬ä¸€è½®å¯¹è¯
    request1 = RunWorkflowRequest(
        workflow="text2sql_enhanced",
        namespace="test_namespace",
        task="æŸ¥è¯¢æœ€è¿‘30å¤©çš„é”€å”®é¢",
        database_name="sales_db",
        include_insights=True
    )

    response1 = client.post("/workflows/run", json=request1.dict())
    assert response1.status_code == 200
    result1 = response1.json()

    # éªŒè¯åŒ…å«æ•°æ®è§£è¯»
    assert "data_interpretation" in result1["data"]

    # ç¬¬äºŒè½®å¯¹è¯ (å¼•ç”¨ä¸Šä¸€è½®ç»“æœ)
    request2 = RunWorkflowRequest(
        workflow="text2sql_enhanced",
        namespace="test_namespace",
        task="æŒ‰åœ°åŒºåˆ†ç»„å±•ç¤º",  # å¼•ç”¨ä¸Šä¸€è½®çš„é”€å”®é¢æ•°æ®
        database_name="sales_db",
        include_insights=True
    )

    response2 = client.post("/workflows/run", json=request2.dict())
    assert response2.status_code == 200
    result2 = response2.json()

    # éªŒè¯å¯¹è¯è®°å¿†èµ·ä½œç”¨
    assert "conversation_memory" in result2["data"]
    assert len(result2["data"]["conversation_memory"]) > 0
```

---

### éªŒæ”¶æ ‡å‡†æ€»ç»“

#### Phase 1: æ·±åº¦åˆ†æèƒ½åŠ›å»ºè®¾
- [ ] æ”¯æŒç«¯åˆ°ç«¯çš„æ·±åº¦åˆ†æå·¥ä½œæµ
- [ ] è‡ªåŠ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š (HTMLæ ¼å¼)
- [ ] åŒ…å«å¯è§†åŒ–å›¾è¡¨ (è‡³å°‘3ç§ç±»å‹: æŠ˜çº¿å›¾ã€æŸ±çŠ¶å›¾ã€é¥¼å›¾)
- [ ] é€šè¿‡10+æµ‹è¯•ç”¨ä¾‹
- [ ] ç»Ÿè®¡æ£€éªŒå‡†ç¡®ç‡ > 90%

#### Phase 2: æ•°æ®è´¨æ£€èƒ½åŠ›å»ºè®¾
- [ ] æ”¯æŒç«¯åˆ°ç«¯çš„æ•°æ®è´¨æ£€å·¥ä½œæµ
- [ ] è‡ªåŠ¨ç”Ÿæˆè´¨æ£€æŠ¥å‘Š (HTMLæ ¼å¼)
- [ ] è´¨é‡è¯„åˆ†å‡†ç¡®æ€§ > 90%
- [ ] é€šè¿‡10+æµ‹è¯•ç”¨ä¾‹
- [ ] è§„èŒƒæ£€æŸ¥è¦†ç›–ç‡ > 95%

#### Phase 3: æ™ºèƒ½é—®æ•°å¢å¼º
- [ ] è‡ªåŠ¨ç”Ÿæˆæ•°æ®æ´å¯Ÿ
- [ ] æ”¯æŒ10+è½®å¯¹è¯è®°å¿†
- [ ] æ´å¯Ÿç›¸å…³æ€§ > 80%
- [ ] å¯¹è¯ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ > 90%

#### Phase 4: SQLç”Ÿæˆ/å®¡æŸ¥ä¼˜åŒ–
- [ ] SQLæ€§èƒ½ä¼˜åŒ–å»ºè®®å‡†ç¡®ç‡ > 85%
- [ ] SQLé‡å†™ç­‰ä»·æ€§ > 95%
- [ ] å®‰å…¨å®¡è®¡è¦†ç›–10+é£é™©ç±»å‹
- [ ] æœ€ä½³å®è·µæ£€æŸ¥è¦†ç›–ç‡ > 90%

#### Phase 5: æ¶æ„ä¼˜åŒ–å’Œç”Ÿæ€å»ºè®¾
- [ ] æ”¯æŒç¬¬ä¸‰æ–¹èŠ‚ç‚¹æ’ä»¶
- [ ] æ’ä»¶çƒ­åŠ è½½ < 5ç§’
- [ ] å¯è§†åŒ–ç¼–è¾‘å™¨å“åº”æ—¶é—´ < 100ms
- [ ] ç›‘æ§è¦†ç›–æ‰€æœ‰èŠ‚ç‚¹
- [ ] å‘Šè­¦å“åº”æ—¶é—´ < 1åˆ†é’Ÿ

---

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†é’ˆå¯¹ Datus 5 å¤§åœºæ™¯çš„è¯¦ç»†æ”¹è¿›æ–¹æ¡ˆï¼ŒåŒ…æ‹¬:

1. **åœºæ™¯ç°çŠ¶åˆ†æ**: è¯¦ç»†åˆ†æäº†5ä¸ªåœºæ™¯çš„å½“å‰å®ç°çŠ¶æ€
2. **æœ€æ–°ä»£ç å®ç°åˆ†æ**: æ·±å…¥è§£æäº† IntentAnalysisNodeã€IntentClarificationNodeã€EnhancedPreflightTools ç­‰æœ€æ–°å®ç°
3. **åœºæ™¯èƒ½åŠ›å·®è·åˆ†æ**: è¯†åˆ«äº†æ·±åº¦åˆ†æå’Œæ•°æ®è´¨æ£€çš„èƒ½åŠ›å·®è·
4. **æ”¹è¿›æ–¹æ¡ˆè®¾è®¡**: è®¾è®¡äº†20ä¸ªæ–°èŠ‚ç‚¹ã€2ä¸ªæ–°å·¥ä½œæµã€APIå¢å¼ºå’ŒPromptæ¨¡æ¿å¢å¼º
5. **å®æ–½è·¯çº¿å›¾**: æä¾›äº†5ä¸ªPhaseçš„è¯¦ç»†å®æ–½è®¡åˆ’ (15-22å‘¨)

**å…³é”®æˆæœ**:
- **æ™ºèƒ½é—®æ•°**: âœ… å·²å®Œæ•´å®ç°ï¼Œå»ºè®®å¢åŠ æ•°æ®è§£è¯»å’Œå¯¹è¯è®°å¿†
- **SQLç”Ÿæˆ**: âœ… å·²å®Œæ•´å®ç°ï¼Œå»ºè®®å¢åŠ æ€§èƒ½ä¼˜åŒ–å’ŒSQLé‡å†™
- **SQLå®¡æŸ¥**: âœ… å·²å®Œæ•´å®ç°ï¼Œå»ºè®®å¢åŠ ä¸šåŠ¡è§„åˆ™éªŒè¯å’Œå®‰å…¨å®¡è®¡
- **æ·±åº¦åˆ†æ**: âš ï¸ éƒ¨åˆ†å®ç°ï¼Œéœ€è¦æ–°å¢5ä¸ªèŠ‚ç‚¹ (ExploratoryAnalysisã€StatisticalAnalysisã€HypothesisTestingã€Visualizationã€ReportGeneration)
- **æ•°æ®è´¨æ£€**: âŒ æœ‰é™å®ç°ï¼Œéœ€è¦æ–°å¢6ä¸ªèŠ‚ç‚¹ (SchemaStandardsCheckã€DataProfilingã€DataQualityCheckã€IndexAnalysisã€DataDriftDetectionã€QualityReport)

**å®æ–½ä¼˜å…ˆçº§**:
- **P0 (æœ€é«˜)**: Phase 1 æ·±åº¦åˆ†æ + Phase 2 æ•°æ®è´¨æ£€ (7-10å‘¨)
- **P1 (é«˜)**: Phase 3 æ™ºèƒ½é—®æ•°å¢å¼º (2-3å‘¨)
- **P2 (ä¸­)**: Phase 4 SQLç”Ÿæˆ/å®¡æŸ¥ä¼˜åŒ– (2-3å‘¨)
- **P3 (ä½)**: Phase 5 æ¶æ„ä¼˜åŒ–å’Œç”Ÿæ€å»ºè®¾ (4-6å‘¨)

é€šè¿‡å®æ–½æœ¬æ”¹è¿›æ–¹æ¡ˆï¼ŒDatus å°†ä»åŸºç¡€çš„SQLå¤„ç†å¹³å°ï¼Œå‡çº§ä¸ºæ”¯æŒå¤æ‚ä¸šåŠ¡åœºæ™¯çš„æ™ºèƒ½åŒ–æ•°æ®åˆ†æå¹³å°ã€‚
